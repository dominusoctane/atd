{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e880e3-7106-49b0-ad3f-c6ed5a195b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0+95.ga4e4894d\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.0.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a4e4894dca25f5e87b9306abfc472805f92b69da\n",
      "MONAI __file__: /usr/local/lib/python3.8/dist-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.21.0\n",
      "scipy version: 1.10.1\n",
      "Pillow version: 10.0.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 4.7.1\n",
      "TorchVision version: 0.15.2+cu117\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.0.3\n",
      "einops version: 0.6.1\n",
      "transformers version: 4.32.1\n",
      "mlflow version: 2.6.0\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from glob import glob\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, PatchDataset\n",
    "from monai.inferers import SliceInferer\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    RandRotate90d,\n",
    "    Resized,\n",
    "    ScaleIntensityd,\n",
    "    SqueezeDimd,\n",
    ")\n",
    "from monai.visualize import matshow3d\n",
    "\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    DeleteItemsd,\n",
    "\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "monai.config.print_config()\n",
    "monai.utils.set_determinism(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d74e20b-25ad-4795-be17-c5c87c6cbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpd7if9faj\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83569ad-2ef3-411b-8f78-32caf7991159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n",
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from monai.transforms import Transform\n",
    "from monai.transforms import SpatialPadd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_bounding_box(data):\n",
    "    \"\"\"\n",
    "    Compute the bounding box of the non-zero region in a multi-dimensional tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): the input data tensor.\n",
    "\n",
    "    Returns:\n",
    "        tuple: a tuple containing the starting and ending coordinates of the bounding box.\n",
    "    \"\"\"\n",
    "    # Find the coordinates of all non-zero data points\n",
    "    coords = torch.nonzero(data)\n",
    "\n",
    "    # Find the minimum and maximum coordinates along each dimension\n",
    "    start_coords = coords.min(dim=0).values\n",
    "    end_coords = coords.max(dim=0).values + 1  # +1 to include the max coordinate\n",
    "\n",
    "    return start_coords.tolist(), end_coords.tolist()\n",
    "\n",
    "\n",
    "class RandCropInsideForeground(Transform):\n",
    "    def __init__(self, roi_key, spatial_size):\n",
    "        self.roi_key = roi_key\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        label_data = data[self.roi_key[0]]\n",
    "        print(label_data.shape)\n",
    "\n",
    "        # Convert label_data to a CUDA tensor\n",
    "        label_data_tensor = torch.tensor(label_data).cuda()\n",
    "\n",
    "        # Ensure the spatial size is within the volume shape\n",
    "        valid_spatial_size = torch.min(torch.tensor(self.spatial_size).cuda(), torch.tensor(label_data.shape[1:]).cuda())\n",
    "\n",
    "        # Convert valid_spatial_size back to a numpy array\n",
    "        valid_spatial_size = valid_spatial_size.cpu().numpy()\n",
    "\n",
    "        # Generate foreground bounding box\n",
    "        fg_start, fg_end = compute_bounding_box(label_data)\n",
    "\n",
    "        # Randomize start and end indices for cropping inside the foreground\n",
    "        start = []\n",
    "        for s, e, sz in zip(fg_start[1:], fg_end[1:], valid_spatial_size):\n",
    "            max_start = max(0, e - sz)\n",
    "            # Use torch.randint for generating random numbers on GPU\n",
    "            start_val = torch.randint(s, max_start + 1, (1,)).item()\n",
    "            start.append(start_val)\n",
    "\n",
    "        self.roi_start = tuple(start)\n",
    "        self.roi_end = tuple([s + sz for s, sz in zip(self.roi_start, valid_spatial_size)])\n",
    "\n",
    "        slices = [slice(s, e) for s, e in zip(self.roi_start, self.roi_end)]\n",
    "        result = {}\n",
    "        for key in data.keys():\n",
    "            if data[key].ndim > 1:\n",
    "                result[key] = data[key][..., slices[0], slices[1], slices[2]]\n",
    "            else:\n",
    "                result[key] = data[key]\n",
    "\n",
    "        print(result['label'].shape)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "from monai.transforms import Transform, SpatialPadd\n",
    "import numpy as np\n",
    "\n",
    "class DynamicAxisPad(Transform):\n",
    "    \"\"\"\n",
    "    Dynamically pad the axis with the smallest size to the desired length.\n",
    "    \"\"\"\n",
    "    def __init__(self, keys, desired_size: int = 96, mode: str = 'constant'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: keys of the corresponding items to be transformed.\n",
    "            desired_size: the desired size after padding.\n",
    "            mode: padding mode, can be one of ['edge', 'constant', 'reflect', 'replicate'].\n",
    "        \"\"\"\n",
    "        self.keys = keys\n",
    "        self.desired_size = desired_size\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for key in self.keys:\n",
    "            img = data[key][0]\n",
    "\n",
    "            # Determine the size of the image\n",
    "            image_shape = img.shape\n",
    "\n",
    "            # Find the axis with the smallest size\n",
    "            min_axis = np.argmin(image_shape)\n",
    "\n",
    "            # Define the padding size based on the minimum axis\n",
    "            padding_size = list(image_shape)  # Assuming a 3D image\n",
    "            padding_size[min_axis] = self.desired_size\n",
    "\n",
    "            # Apply the padding\n",
    "            padder = SpatialPadd(keys=[key], spatial_size=padding_size, mode=self.mode)\n",
    "            data = padder(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=False),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        DeleteItemsd(keys=[\"image_meta_dict\", \"label_meta_dict\",\"foreground_start_coord\", \"foreground_end_coord\"]),  # Remove the metadata\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True,),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "        # DynamicAxisPad(keys=['image'], desired_size=96, mode='constant'),\n",
    "        # RandCropInsideForeground(roi_key=[\"label\"], spatial_size=(96, 96, 96)),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# val_transforms = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\"], ensure_channel_first=False),\n",
    "#         EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "#         DeleteItemsd(keys=[\"image_meta_dict\", \"label_meta_dict\",\"foreground_start_coord\", \"foreground_end_coord\"]),  # Remove the metadata\n",
    "#         ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "#         DynamicAxisPad(keys=['image'], desired_size=96, mode='constant'),\n",
    "#         CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "#         Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#         Spacingd(\n",
    "#             keys=[\"image\", \"label\"],\n",
    "#             pixdim=(1.5, 1.5, 2.0),\n",
    "#             mode=(\"bilinear\", \"nearest\"),\n",
    "#         ),\n",
    "#         EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6534ece-ffd3-46a8-b98c-737bac5d9548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8c56dc-ccd0-4a66-ae66-252d82faf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from monai.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "\n",
    "split_json = \"output_1.json\"\n",
    "\n",
    "datasets = os.path.join(current_directory, split_json)\n",
    "train_files = load_decathlon_datalist(datasets, True, \"training\")\n",
    "val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "\n",
    "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "# train_loader = DataLoader(train_ds, num_workers=0, batch_size=1, shuffle=True)\n",
    "\n",
    "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "# val_loader = DataLoader(val_ds, num_workers=0, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e185b1d8-f70d-4a22-8f60-f2391f025260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|█████████████████████████████████████| 8/8 [00:15<00:00,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first volume's shape:  torch.Size([1, 1, 117, 243, 256]) torch.Size([1, 1, 117, 243, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# volume-level transforms for both image and segmentation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        DynamicAxisPad(keys=['image'], desired_size=96, mode='constant'),\n",
    "        ScaleIntensityd(keys=\"image\"),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True,),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "# 3D dataset with preprocessing transforms\n",
    "volume_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms, cache_num=8,cache_rate=1,num_workers=8)\n",
    "# use batch_size=1 to check the volumes because the input volumes have different shapes\n",
    "check_loader = DataLoader(volume_ds, batch_size=1)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(\"first volume's shape: \", check_data[\"image\"].shape, check_data[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b53bea-9fce-4b3f-b52e-894d678fc67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first patch's shape:  torch.Size([3, 1, 48, 48]) torch.Size([3, 1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "num_samples = 4\n",
    "patch_func = monai.transforms.RandSpatialCropSamplesd(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    roi_size=[1, -1, -1],  # dynamic spatial_size for the first two dimensions\n",
    "    num_samples=num_samples,\n",
    "    random_size=False,\n",
    ")\n",
    "\n",
    "patch_transform = Compose(\n",
    "    [\n",
    "        SqueezeDimd(keys=[\"image\", \"label\"], dim=-1),  # squeeze the last dim\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=[96, 96]),\n",
    "        # to use crop/pad instead of resize:\n",
    "        # ResizeWithPadOrCropd(keys=[\"img\", \"seg\"], spatial_size=[48, 48], mode=\"replicate\"),\n",
    "    ]\n",
    ")\n",
    "patch_ds = PatchDataset(\n",
    "    volume_ds,\n",
    "    transform=patch_transform,\n",
    "    patch_func=patch_func,\n",
    "    samples_per_image=num_samples,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    patch_ds,\n",
    "    batch_size=3,\n",
    "    shuffle=True,  # this shuffles slices from different volumes\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "check_data = monai.utils.misc.first(train_loader)\n",
    "print(\"first patch's shape: \", check_data[\"image\"].shape, check_data[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6cc398-1937-4505-8f44-03e3bb75b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=6,\n",
    "    channels=(16, 32, 64, 128),\n",
    "    strides=(2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "#loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "loss_function = monai.losses.DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), 5e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9191a-4c15-4dcc-99f1-1e0369e9443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_values = []\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss, step = 0, 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "\n",
    "        # with torch.cuda.amp.autocast():\n",
    "        #     outputs = model(inputs)\n",
    "        #     loss = loss_function(outputs, labels)\n",
    "        # scaler.scale(loss).backward()\n",
    "        # epoch_loss += loss.item()\n",
    "        # scaler.unscale_(optimizer)\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        # optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        # outputs = model(inputs)\n",
    "        # loss = loss_function(outputs, labels)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # epoch_loss += loss.item()\n",
    "\n",
    "        \n",
    "        epoch_len = len(patch_ds) // train_loader.batch_size\n",
    "        if step % 25 == 0:\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "print(\"train completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
