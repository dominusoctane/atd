{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e349a7d-8d54-46d2-aeb7-672ebecb702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-04 21:06:11.355536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "In the future `np.object` will be defined as the corresponding NumPy scalar.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    ThreadDataLoader,\n",
    "    Dataset,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.layers import Act, Norm\n",
    "from monai.networks.nets import UNet, SwinUNETR, AHNet, VNet\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    EnsureTyped,\n",
    "    FgBgToIndicesd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "# for profiling\n",
    "import nvtx\n",
    "from monai.utils.nvtx import Range\n",
    "import contextlib  # to improve code readability (combining training/validation loop with and without profiling)\n",
    "\n",
    "#print_config()\n",
    "set_determinism(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9068c62-e1ea-4ca3-91d9-3c6a69baf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir is: /tmp/tmpq6bd39wr\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(f\"root dir is: {root_dir}\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "out_dir = os.path.join(current_directory,\"outputs/\")\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40299e96-acdd-4076-ab77-42480326b32c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import albumentations\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Optional, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from timm.layers.conv2d_same import Conv2dSame\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "# Calculate symmetric padding for a convolution\n",
    "def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n",
    "    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n",
    "    return padding\n",
    "\n",
    "\n",
    "# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n",
    "def get_same_padding(x: int, k: int, s: int, d: int):\n",
    "    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
    "\n",
    "\n",
    "# Can SAME padding for given args be done statically?\n",
    "def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n",
    "    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n",
    "\n",
    "\n",
    "# Dynamically pad input x with 'SAME' padding for conv with specified args\n",
    "def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1, 1), value: float = 0):\n",
    "    ih, iw, iz = x.size()[-3:]\n",
    "    pad_h = get_same_padding(ih, k[0], s[0], d[0])\n",
    "    pad_w = get_same_padding(iw, k[1], s[1], d[1])\n",
    "    pad_z = get_same_padding(iz, k[2], s[2], d[2])\n",
    "    if pad_h > 0 or pad_w > 0 or pad_z > 0:\n",
    "        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2, pad_z // 2, pad_z - pad_z // 2], value=value)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n",
    "    dynamic = False\n",
    "    if isinstance(padding, str):\n",
    "        # for any string padding, the padding will be calculated for you, one of three ways\n",
    "        padding = padding.lower()\n",
    "        if padding == 'same':\n",
    "            # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact\n",
    "            if is_static_pad(kernel_size, **kwargs):\n",
    "                # static case, no extra overhead\n",
    "                padding = get_padding(kernel_size, **kwargs)\n",
    "            else:\n",
    "                # dynamic 'SAME' padding, has runtime/GPU memory overhead\n",
    "                padding = 0\n",
    "                dynamic = True\n",
    "        elif padding == 'valid':\n",
    "            # 'VALID' padding, same as padding=0\n",
    "            padding = 0\n",
    "        else:\n",
    "            # Default to PyTorch style 'same'-ish symmetric padding\n",
    "            padding = get_padding(kernel_size, **kwargs)\n",
    "    return padding, dynamic\n",
    "\n",
    "\n",
    "def conv3d_same(\n",
    "        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int, int] = (1, 1, 1),\n",
    "        padding: Tuple[int, int, int] = (0, 0, 0), dilation: Tuple[int, int, int] = (1, 1, 1), groups: int = 1):\n",
    "    x = pad_same(x, weight.shape[-3:], stride, dilation)\n",
    "    return F.conv3d(x, weight, bias, stride, (0, 0, 0), dilation, groups)\n",
    "\n",
    "\n",
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3d convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def create_conv3d_pad(in_chs, out_chs, kernel_size, **kwargs):\n",
    "    padding = kwargs.pop('padding', '')\n",
    "    kwargs.setdefault('bias', False)\n",
    "    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n",
    "    if is_dynamic:\n",
    "        return Conv3dSame(in_chs, out_chs, kernel_size, **kwargs)\n",
    "    else:\n",
    "        return nn.Conv3d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)\n",
    "\n",
    "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "n_folds = 5\n",
    "backbone = 'resnet18d'\n",
    "\n",
    "init_lr = 3e-3\n",
    "batch_size = 4\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "out_dim = 6\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False, num_classes=6):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.1,\n",
    "            drop_path_rate=0.1,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        # Modify the first layer of the encoder for 1-channel input\n",
    "        self.encoder.conv1 = nn.Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
    "\n",
    "        self.decoder = smp.Unet(\n",
    "            encoder_name=backbone,\n",
    "            encoder_depth=5,\n",
    "            encoder_weights=None,  # Weights are already loaded in the encoder part\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=[256, 128, 64, 32, 16],\n",
    "            decoder_attention_type='scse',\n",
    "            in_channels=1,\n",
    "            classes=num_classes,\n",
    "            activation=None,  # Use 'None' because we handle activation in the loss function (e.g., CrossEntropy)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfc94a06-f1f5-4776-9520-5e86d8953287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the SEGMENTATION_CODES dictionary and mock data\n",
    "SEGMENTATION_CODES = {\n",
    "    #0: 'background',\n",
    "    1: 'liver',\n",
    "    2: 'spleen',\n",
    "    3: 'kidney_left',\n",
    "    4: 'kidney_right',\n",
    "    5: 'bowel',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48821b80-40f3-45c9-b801-9f792a1818b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_get_bounding_boxes_v6(tensor):\n",
    "    \"\"\"\n",
    "    Compute bounding boxes, determine the voxel count per depth, and the inner region for each organ.\n",
    "    Also computes a bounding box that encompasses all organs based on the inner_slices.\n",
    "    \n",
    "    Args:\n",
    "    - tensor (numpy.ndarray): The input tensor with shape (D, H, W).\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Bounding boxes, voxel counts per depth, and inner regions for each organ.\n",
    "            Also includes the encompassing bounding box for all organs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize bounding boxes dictionary\n",
    "    bounding_boxes = {}\n",
    "    \n",
    "    # List to store all coordinates within the inner_slices for all organs\n",
    "    all_coords_in_inner_slices = []\n",
    "    \n",
    "    for organ_label, organ_name in SEGMENTATION_CODES.items():\n",
    "        if organ_label == 0:  # Skip background\n",
    "            continue\n",
    "\n",
    "        # Fourier-based smoothing logic based on organ_name\n",
    "        fourier_factor = 0\n",
    "        threshold_factor = 0\n",
    "        if organ_name == 'liver':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.2\n",
    "        elif organ_name == 'spleen':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.3\n",
    "        elif organ_name == 'kidney_left':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.3\n",
    "        elif organ_name == 'kidney_right':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.35\n",
    "        elif organ_name == 'bowel':\n",
    "            fourier_factor = 0.025\n",
    "            threshold_factor = 0.2\n",
    "        \n",
    "        # Find the slices where the organ appears\n",
    "        #print(tensor.shape)\n",
    "        # Find the slices where the organ appears\n",
    "        slices = np.where(np.any(tensor[0] == organ_label, axis=(1, 2)))[0]\n",
    "        \n",
    "        print(slices.shape)\n",
    "        if len(slices) == 0:  # If organ does not appear, continue to next organ\n",
    "            continue\n",
    "        \n",
    "        # Compute voxel count for each slice and store in a dictionary with slice index as key\n",
    "        voxel_counts_per_depth = {slice_idx: np.sum(tensor[0, slice_idx] == organ_label) for slice_idx in slices}\n",
    "        \n",
    "        # Fourier-based smoothing and thresholding\n",
    "        simple_list = list(voxel_counts_per_depth.values())\n",
    "        smoothed_list = fourier_smoothing(simple_list, fourier_factor)\n",
    "        \n",
    "        # Thresholding logic\n",
    "        max_value = np.max(smoothed_list)\n",
    "        threshold = threshold_factor * max_value\n",
    "        in_threshold = np.where(np.array(smoothed_list) >= threshold)[0].tolist()\n",
    "\n",
    "        # Extract slice indices that are in the inner region\n",
    "        #inner_slices = get_contiguous_inner_slices(in_threshold, slices)\n",
    "        \n",
    "        # Gather all coordinates for this organ that are within the inner_slices\n",
    "        coords = np.argwhere((tensor[0] == organ_label) & np.isin(tensor[0], in_threshold))\n",
    "        print(coords.shape)\n",
    "        all_coords_in_inner_slices.extend(coords)\n",
    "        \n",
    "        # Bounding box computation for this organ\n",
    "        min_coords = coords.min(axis=0)\n",
    "        max_coords = coords.max(axis=0)\n",
    "        \n",
    "        bounding_boxes[organ_name] = {\n",
    "            \"top_left_front\": tuple(min_coords),\n",
    "            \"bottom_right_back\": tuple(max_coords),\n",
    "            \"depth_range\": (min_coords[0], max_coords[0]),\n",
    "            \"voxel_counts_per_depth\": voxel_counts_per_depth,\n",
    "            \"inner_slices\": inner_slices\n",
    "        }\n",
    "    \n",
    "    # Compute the encompassing bounding box for all organs based on inner_slices\n",
    "    print(all_coords_in_inner_slices.shape)\n",
    "    \n",
    "    all_coords_array = np.array(all_coords_in_inner_slices)\n",
    "    min_encompassing_coords = all_coords_array.min(axis=0)\n",
    "    max_encompassing_coords = all_coords_array.max(axis=0)\n",
    "    bounding_boxes[\"encompassing\"] = {\n",
    "        \"top_left_front\": tuple(min_encompassing_coords),\n",
    "        \"bottom_right_back\": tuple(max_encompassing_coords),\n",
    "        \"depth_range\": (min_encompassing_coords[0], max_encompassing_coords[0])\n",
    "    }\n",
    "        \n",
    "    return bounding_boxes\n",
    "\n",
    "\n",
    "def fourier_smoothing(y, cutoff_fraction=0.1):\n",
    "    # Compute the FFT of the input data\n",
    "    y_fft = np.fft.fft(y)\n",
    "    \n",
    "    # Zero out the higher frequencies\n",
    "    cutoff_idx = int(cutoff_fraction * len(y_fft))\n",
    "    y_fft[cutoff_idx:-cutoff_idx] = 0\n",
    "    \n",
    "    # Compute the inverse FFT to get the smoothed data\n",
    "    y_smoothed = np.fft.ifft(y_fft).real  # Only take the real part\n",
    "    \n",
    "    return y_smoothed\n",
    "    \n",
    "def get_contiguous_inner_slices(in_threshold, slices):\n",
    "    \"\"\"\n",
    "    Get contiguous blocks of slices that are above the threshold.\n",
    "    \n",
    "    Args:\n",
    "    - in_threshold (list): A boolean list indicating if the slice is above the threshold.\n",
    "    - slices (list): List of slice indices.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Contiguous inner slices.\n",
    "    \"\"\"\n",
    "    inner_slices = []\n",
    "    start_slice = None\n",
    "\n",
    "    for i, val in enumerate(in_threshold):\n",
    "        if val:\n",
    "            if start_slice is None:  # Start of a new block\n",
    "                start_slice = slices[i]\n",
    "        else:\n",
    "            if start_slice is not None:  # End of the current block\n",
    "                inner_slices.extend(range(start_slice, slices[i]))\n",
    "                start_slice = None\n",
    "\n",
    "    if start_slice is not None:  # For the case where the last block reaches the end\n",
    "        inner_slices.extend(range(start_slice, slices[-1] + 1))\n",
    "\n",
    "    return inner_slices\n",
    "\n",
    "# Function to save numpy array as .nii.gz\n",
    "def save_nii(data, filename):\n",
    "    img = nib.Nifti1Image(data, affine=np.eye(4))\n",
    "    nib.save(img, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f6a9bb7-7e2a-4355-98c1-60d69d694f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def squash(inputs, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param inputs: vectors to be squashed\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same size as inputs\n",
    "    \"\"\"\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs\n",
    "\n",
    "\n",
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
    "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
    "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
    "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
    "\n",
    "    :param in_num_caps: number of cpasules inputted to this layer\n",
    "    :param in_dim_caps: dimension of input capsules\n",
    "    :param out_num_caps: number of capsules outputted from this layer\n",
    "    :param out_dim_caps: dimension of output capsules\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size=[batch, in_num_caps, in_dim_caps]\n",
    "        # expanded to    [batch, 1,            in_num_caps, in_dim_caps,  1]\n",
    "        # weight.size   =[       out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]\n",
    "        # torch.matmul: [out_dim_caps, in_dim_caps] x [in_dim_caps, 1] -> [out_dim_caps, 1]\n",
    "        # => x_hat.size =[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "\n",
    "        # In forward pass, `x_hat_detached` = `x_hat`;\n",
    "        # In backward, no gradient can flow from `x_hat_detached` back to `x_hat`.\n",
    "        x_hat_detached = x_hat.detach()\n",
    "\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        #b = Variable(torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)).cuda()\n",
    "        b = Variable(torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)).to(x.device)\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)\n",
    "\n",
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
    "        return squash(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31c536e8-3596-4049-8c53-58395bd0e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encompassing_subregion(volume, bounding_boxes):\n",
    "    # Extract the coordinates from the 'encompassing' bounding box\n",
    "    top_left_front = bounding_boxes[\"encompassing\"][\"top_left_front\"]\n",
    "    bottom_right_back = bounding_boxes[\"encompassing\"][\"bottom_right_back\"]\n",
    "\n",
    "    # Use the coordinates to slice into the volume tensor\n",
    "    subregion = volume[top_left_front[0]:bottom_right_back[0]+1, \n",
    "                       top_left_front[1]:bottom_right_back[1]+1, \n",
    "                       top_left_front[2]:bottom_right_back[2]+1]\n",
    "\n",
    "    return subregion\n",
    "\n",
    "def preprocess_and_extract_torch(volume, model, device, resize_to=(100, 224, 224), use_three_channel=True, overlap=True):\n",
    "    \"\"\"\n",
    "    Given a 3D volume, this function will preprocess each slice and extract features using the provided model.\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for CUDA availability and set the device accordingly\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Convert ndarray to tensor and normalize to [0, 1] range\n",
    "    if isinstance(volume, np.ndarray):\n",
    "        volume = torch.from_numpy(volume.astype(np.float32)).to(device)\n",
    "        min_val = torch.min(volume)\n",
    "        max_val = torch.max(volume)\n",
    "        volume = (volume - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Normalization parameters\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1)\n",
    "\n",
    "    # Resize the slices\n",
    "    #volume = torch.nn.functional.interpolate(volume, size=resize_to, mode='trilinear', align_corners=True)\n",
    "    volume = torch.nn.functional.interpolate(volume.unsqueeze(0).unsqueeze(0), size=resize_to, mode='trilinear', align_corners=True).squeeze(0).squeeze(0)\n",
    "\n",
    "    # Define the step based on whether overlap is desired\n",
    "    step = 1 if overlap else 3\n",
    "\n",
    "    # Iterate through the volume\n",
    "    depth = volume.shape[0]\n",
    "    features_list = []\n",
    "\n",
    "    for idx in tqdm(range(1, depth-1, step)):  # Avoiding starting at index 0 and ending at index -1\n",
    "        if use_three_channel:\n",
    "            # Create 3-channel image\n",
    "            slices = volume[max(idx-1, 0):min(idx+2, depth), :, :]\n",
    "            # Normalize the 3-channel image\n",
    "            normalized_image = (slices - mean) / std\n",
    "            normalized_image = normalized_image.unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Extract features and append to the list\n",
    "            with torch.no_grad():\n",
    "                features = model(normalized_image)\n",
    "\n",
    "            #features = features.detach().cpu().numpy()\n",
    "            features = [f.detach().cpu().numpy() for f in features]\n",
    "\n",
    "            features_list.append(features)\n",
    "\n",
    "            del normalized_image\n",
    "            \n",
    "    # Delete any unnecessary data to free up CUDA memory\n",
    "    del mean, std, volume\n",
    "    # Free up CUDA memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2438b2-ef87-4424-bbd6-4cec80c07cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patients: 100%|███████████████████████| 3147/3147 [00:00<00:00, 122965.40it/s]\n",
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the path to the train_images directory located one level up\n",
    "train_base_directory = os.path.join(current_directory, 'train_images')\n",
    "\n",
    "# Collect all the DICOM file paths along with patient IDs and series ID\n",
    "dicom_data = []\n",
    "for patient_id in tqdm(os.listdir(train_base_directory), desc=\"Processing Patients\"):\n",
    "    patient_dir = os.path.join(train_base_directory, patient_id)\n",
    "    for series_id in os.listdir(patient_dir):\n",
    "        series_dir = os.path.join(patient_dir, series_id)\n",
    "        dicom_data.append({'series_dir': series_dir, 'patient_id': patient_id, 'series_id': series_id})\n",
    "\n",
    "# Create a DataFrame using the collected data\n",
    "dicom_df = pd.DataFrame(dicom_data)\n",
    "\n",
    "dicom_df['patient_duplicate_count'] = dicom_df.groupby('patient_id')['patient_id'].transform('count')\n",
    "\n",
    "# Transformation pipeline\n",
    "test_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=\"image\"),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(2.0, 2.0, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa14761-98a3-4d43-8022-782fcc07d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free RAM: 47795351552 bytes\n",
      "Free RAM: 44.51288986206055 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "# Get the virtual memory status\n",
    "memory_info = psutil.virtual_memory()\n",
    "\n",
    "# Print the free memory in bytes\n",
    "print(f\"Free RAM: {memory_info.free} bytes\")\n",
    "\n",
    "# If you want the free RAM in GB\n",
    "print(f\"Free RAM: {memory_info.free / (1024 ** 3)} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "362845d3-9f19-4ec6-8b8c-78d0e60c516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           125G        9.9G         44G        155M         71G        114G\n",
      "Swap:          4.0G        690M        3.3G\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51516f1-d963-481f-9413-fdd97e5ea48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, pretrained=True, unet_feature_dim=1024, capsnet_feature_dim=1024):\n",
    "        super(CombinedFeatureExtractor, self).__init__()\n",
    "\n",
    "        # Use pre-trained ResNet as encoder but modify for desired input channels\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # Remove the classification head\n",
    "\n",
    "        # Decoder (U-Net style)\n",
    "        self.decoder_block1 = self.conv_block(512, 256)\n",
    "        self.decoder_block2 = self.conv_block(256, 128)\n",
    "        self.decoder_block3 = self.conv_block(128, 64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, input_channels, 256, 256)\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder_block1(F.interpolate(x, scale_factor=2))\n",
    "            x = self.decoder_block2(F.interpolate(x, scale_factor=2))\n",
    "            x = self.decoder_block3(F.interpolate(x, scale_factor=2))\n",
    "        \n",
    "        # Calculate the number of features and initialize the fully connected layer\n",
    "        num_features = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        # Calculate the number of features for U-Net and initialize the fully connected layer\n",
    "        self.fc_unet = nn.Linear(num_features, unet_feature_dim)  # Assuming 256x256 input size\n",
    "        \n",
    "        # CapsNet Layer\n",
    "        self.primary_capsules = PrimaryCapsule(in_channels=512, out_channels=256, dim_caps=8, kernel_size=1)\n",
    "\n",
    "        self.digit_capsules = DenseCapsule(in_num_caps=2048, \n",
    "                                           in_dim_caps=self.primary_capsules.dim_caps,\n",
    "                                           out_num_caps=capsnet_feature_dim // 8,  # Adjust the number of capsules based on desired feature dimension\n",
    "                                           out_dim_caps=8, \n",
    "                                           routings=3)\n",
    "        \n",
    "        self.fc_caps = nn.Linear(capsnet_feature_dim, capsnet_feature_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (pre-trained CNN)\n",
    "        encoded_x = self.encoder(x)\n",
    "\n",
    "        # Decoder (U-Net)\n",
    "        unet_features = self.decoder_block1(F.interpolate(encoded_x, scale_factor=2))\n",
    "        unet_features = self.decoder_block2(F.interpolate(unet_features, scale_factor=2))\n",
    "        unet_features = self.decoder_block3(F.interpolate(unet_features, scale_factor=2))\n",
    "        \n",
    "        # Flatten and pass through the fully connected layer for 1D feature vector\n",
    "        unet_features = unet_features.view(unet_features.size(0), -1)\n",
    "        unet_features = self.fc_unet(unet_features)\n",
    " \n",
    "        x_reshaped = encoded_x.view(encoded_x.size(0), 512, 1, 1)\n",
    "        x_reshaped = F.interpolate(x_reshaped, size=(8, 8), mode='nearest')  # Upsample to [batch_size, 512, 7, 7]\n",
    "\n",
    "        caps_features = self.primary_capsules(x_reshaped)\n",
    "\n",
    "        caps_features = self.digit_capsules(caps_features)\n",
    "        caps_features = self.fc_caps(caps_features.view(caps_features.size(0), -1))\n",
    "\n",
    "        # Combine the features from U-Net and CapsNet\n",
    "        combined_features = torch.cat([unet_features, caps_features], dim=1)\n",
    "\n",
    "        return unet_features, caps_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f251d6-40fd-4176-b2f6-f621bd9db24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom as dicom\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from scipy.ndimage import zoom\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def adjust_pixel_values(pixel_array, slope, intercept, center, width):\n",
    "    pixel_array = (pixel_array * slope) + intercept\n",
    "    low = center - width / 2\n",
    "    high = center + width / 2\n",
    "    pixel_array = np.clip(pixel_array, low, high)\n",
    "    pixel_array -= np.min(pixel_array)\n",
    "    pixel_array = (pixel_array / np.max(pixel_array) * 255).astype(np.int16)\n",
    "    return pixel_array\n",
    "\n",
    "\n",
    "def sample_patient_volume(folder, depth_downsample_rate=None, lw_downsample_rate=None, \n",
    "                          adjust_pixel_spacing=\"no\", standardize_pixel_array=\"yes\", \n",
    "                          target_pixel_spacing=[1.0, 1.0], target_thickness=None):\n",
    "    \"\"\"\n",
    "    Load and optionally adjust and standardize the pixel array from DICOM metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = sorted([int(filename.split('.')[0]) for filename in os.listdir(folder)])\n",
    "    filenames = [str(filename) + '.dcm' for filename in filenames]\n",
    "    \n",
    "    volume = []\n",
    "    original_thicknesses = []\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        dcm = dicom.dcmread(filepath)\n",
    "        original_thicknesses.append(float(dcm.SliceThickness))\n",
    "        \n",
    "        pixel_array = dcm.pixel_array\n",
    "\n",
    "        # Adjust pixel spacing if required\n",
    "        if standardize_pixel_array.lower() == \"yes\":\n",
    "            if dcm.PixelRepresentation == 1:\n",
    "                bit_shift = dcm.BitsAllocated - dcm.BitsStored\n",
    "                dtype = pixel_array.dtype \n",
    "                pixel_array = (pixel_array << bit_shift).astype(dtype) >> bit_shift\n",
    "\n",
    "            intercept = float(dcm.RescaleIntercept)\n",
    "            slope = float(dcm.RescaleSlope)\n",
    "            center = int(dcm.WindowCenter)\n",
    "            width = int(dcm.WindowWidth)\n",
    "            pixel_array = adjust_pixel_values(pixel_array, slope, intercept, center, width)\n",
    "            \n",
    "        if adjust_pixel_spacing.lower() == \"yes\":\n",
    "            row_spacing, col_spacing = dcm.PixelSpacing\n",
    "            new_shape = (int(pixel_array.shape[0] * (row_spacing/target_pixel_spacing[0])), \n",
    "                         int(pixel_array.shape[1] * (col_spacing/target_pixel_spacing[1])))\n",
    "            pixel_array = resize(pixel_array, new_shape)\n",
    "\n",
    "        if lw_downsample_rate:\n",
    "            pixel_array = pixel_array[::lw_downsample_rate, ::lw_downsample_rate]\n",
    "\n",
    "        volume.append(pixel_array)\n",
    "        \n",
    "    volume = np.array(volume)\n",
    "    print(volume.shape)\n",
    "    # Adjust the slice thickness if needed\n",
    "    if target_thickness is not None:\n",
    "        original_thickness = np.mean(original_thicknesses)\n",
    "        z_zoom_factor = original_thickness / target_thickness\n",
    "        volume = zoom(volume, (z_zoom_factor, 1, 1), order=1)  # order=1 for bilinear interpolation\n",
    "        \n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c369615e-2330-43e5-8219-08fa854bc66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedFeatureExtractor(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (decoder_block1): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_block2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_block3): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_unet): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (primary_capsules): PrimaryCapsule(\n",
       "    (conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (digit_capsules): DenseCapsule()\n",
       "  (fc_caps): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "train_info = pd.read_csv('train.csv')\n",
    "train_info.patient_id = train_info.patient_id.astype(str)\n",
    "\n",
    "curr_df = dicom_df[dicom_df.patient_duplicate_count == 2]\n",
    "train_info = train_info[train_info.patient_id.isin(curr_df.patient_id)]\n",
    "train_info = train_info[train_info.extravasation_injury == 1]\n",
    "curr_df = curr_df[curr_df.patient_id.isin(train_info.patient_id)]\n",
    "\n",
    "unique_patients = np.unique(curr_df.patient_id)\n",
    "\n",
    "# Directory where the .nii.gz files are saved\n",
    "vol_directory = \"/workspace/0728tot/ATD/patient_nii\"\n",
    "\n",
    "\n",
    "seg_model_device = torch.device(\"cuda:0\")\n",
    "feature_extractor_device = torch.device(\"cuda:1\")\n",
    "\n",
    "# Create the model\n",
    "seg_model = TimmSegModel(backbone = 'resnet18', segtype='unet', pretrained = True)\n",
    "seg_model = convert_3d(seg_model)\n",
    "# Test the model (just to ensure everything is working)\n",
    "seg_model = seg_model.to(seg_model_device)\n",
    "\n",
    "seg_model.load_state_dict(torch.load(os.path.join(current_directory, \"resnet18_unet_best_metric_model.pt\")))\n",
    "seg_model.eval()\n",
    "\n",
    "\n",
    "feature_extractor = CombinedFeatureExtractor(input_channels = 3, pretrained=True, unet_feature_dim=1024, capsnet_feature_dim=1024)\n",
    "feature_extractor = feature_extractor.to(feature_extractor_device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c330c6f6-5565-4487-b632-fa6f808e7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_get_bounding_boxes_v6(tensor):\n",
    "    \"\"\"\n",
    "    Compute bounding boxes, determine the voxel count per depth, and the inner region for each organ.\n",
    "    Also computes a bounding box that encompasses all organs based on the inner_slices.\n",
    "    \n",
    "    Args:\n",
    "    - tensor (numpy.ndarray): The input tensor with shape (D, H, W).\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Bounding boxes, voxel counts per depth, and inner regions for each organ.\n",
    "            Also includes the encompassing bounding box for all organs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize bounding boxes dictionary\n",
    "    bounding_boxes = {}\n",
    "    \n",
    "    # List to store all coordinates within the inner_slices for all organs\n",
    "    all_coords_in_inner_slices = []\n",
    "    \n",
    "    for organ_label, organ_name in SEGMENTATION_CODES.items():\n",
    "        if organ_label == 0:  # Skip background\n",
    "            continue\n",
    "\n",
    "        # Fourier-based smoothing logic based on organ_name\n",
    "        fourier_factor = 0\n",
    "        threshold_factor = 0\n",
    "        if organ_name == 'liver':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.2\n",
    "        elif organ_name == 'spleen':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.3\n",
    "        elif organ_name == 'kidney_left':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.3\n",
    "        elif organ_name == 'kidney_right':\n",
    "            fourier_factor = 0.05\n",
    "            threshold_factor = 0.35\n",
    "        elif organ_name == 'bowel':\n",
    "            fourier_factor = 0.025\n",
    "            threshold_factor = 0.2\n",
    "        \n",
    "        # Find the slices where the organ appears\n",
    "        slices = np.where(np.any(tensor[0] == organ_label, axis=(1, 2)))[0]\n",
    "        \n",
    "        if len(slices) == 0:  # If organ does not appear, continue to next organ\n",
    "            continue\n",
    "        \n",
    "        # Compute voxel count for each slice and store in a dictionary with slice index as key\n",
    "        voxel_counts_per_depth = {slice_idx: np.sum(tensor[0, slice_idx] == organ_label) for slice_idx in slices}\n",
    "        \n",
    "        # Fourier-based smoothing and thresholding\n",
    "        # simple_list = list(voxel_counts_per_depth.values())\n",
    "        # smoothed_list = fourier_smoothing(simple_list, fourier_factor)\n",
    "        \n",
    "        # # Thresholding logic\n",
    "        # max_value = np.max(smoothed_list)\n",
    "        # threshold = threshold_factor * max_value\n",
    "        # in_threshold = np.where(np.array(smoothed_list) >= threshold)[0].tolist()\n",
    "\n",
    "        # Fourier-based smoothing\n",
    "        depths = list(voxel_counts_per_depth.keys())\n",
    "        values = list(voxel_counts_per_depth.values())\n",
    "        smoothed_values = fourier_smoothing(values, fourier_factor)\n",
    "        voxel_counts_smoothed = dict(zip(depths, smoothed_values))\n",
    "        \n",
    "        # Thresholding logic\n",
    "        max_value = np.max(smoothed_values)\n",
    "        threshold = threshold_factor * max_value\n",
    "        in_threshold = [depth for depth, value in voxel_counts_smoothed.items() if value >= threshold]\n",
    "        # Extract slice indices that are in the inner region\n",
    "        # inner_slices = get_contiguous_inner_slices(in_threshold, slices)\n",
    "\n",
    "        # Gather all coordinates for this organ that are within the inner_slices\n",
    "        # coords = np.argwhere((tensor[0] == organ_label) & np.isin(tensor[0], in_threshold))\n",
    "        depth_mask = np.isin(np.arange(tensor.shape[1]), in_threshold)\n",
    "        organ_mask = tensor[0] == organ_label\n",
    "        combined_mask = np.logical_and(organ_mask, depth_mask[:, None, None])\n",
    "\n",
    "        \n",
    "        coords = np.argwhere(combined_mask)\n",
    "\n",
    "        all_coords_in_inner_slices.extend(coords)\n",
    "        \n",
    "        # Bounding box computation for this organ\n",
    "        min_coords = coords.min(axis=0)\n",
    "        max_coords = coords.max(axis=0)\n",
    "        \n",
    "        bounding_boxes[organ_name] = {\n",
    "            \"top_left_front\": tuple(min_coords),\n",
    "            \"bottom_right_back\": tuple(max_coords),\n",
    "            \"depth_range\": (min_coords[0], max_coords[0]),\n",
    "            \"voxel_counts_per_depth\": voxel_counts_per_depth,\n",
    "            \"inner_slices\": in_threshold\n",
    "        }\n",
    "    \n",
    "    # Compute the encompassing bounding box for all organs based on inner_slices\n",
    "    all_coords_array = np.array(all_coords_in_inner_slices)\n",
    "\n",
    "    min_encompassing_coords = all_coords_array.min(axis=0)\n",
    "    max_encompassing_coords = all_coords_array.max(axis=0)\n",
    "    bounding_boxes[\"encompassing\"] = {\n",
    "        \"top_left_front\": tuple(min_encompassing_coords),\n",
    "        \"bottom_right_back\": tuple(max_encompassing_coords),\n",
    "        \"depth_range\": (min_encompassing_coords[0], max_encompassing_coords[0])\n",
    "    }\n",
    "        \n",
    "    return bounding_boxes\n",
    "\n",
    "\n",
    "def fourier_smoothing(y, cutoff_fraction=0.1):\n",
    "    # Compute the FFT of the input data\n",
    "    y_fft = np.fft.fft(y)\n",
    "    \n",
    "    # Zero out the higher frequencies\n",
    "    cutoff_idx = int(cutoff_fraction * len(y_fft))\n",
    "    y_fft[cutoff_idx:-cutoff_idx] = 0\n",
    "    \n",
    "    # Compute the inverse FFT to get the smoothed data\n",
    "    y_smoothed = np.fft.ifft(y_fft).real  # Only take the real part\n",
    "    \n",
    "    return y_smoothed\n",
    "    \n",
    "def get_contiguous_inner_slices(in_threshold, slices):\n",
    "    \"\"\"\n",
    "    Get contiguous blocks of slices that are above the threshold.\n",
    "    \n",
    "    Args:\n",
    "    - in_threshold (list): A boolean list indicating if the slice is above the threshold.\n",
    "    - slices (list): List of slice indices.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Contiguous inner slices.\n",
    "    \"\"\"\n",
    "    inner_slices = []\n",
    "    start_slice = None\n",
    "\n",
    "    for i, val in enumerate(in_threshold):\n",
    "        if val:\n",
    "            if start_slice is None:  # Start of a new block\n",
    "                start_slice = slices[i]\n",
    "        else:\n",
    "            if start_slice is not None:  # End of the current block\n",
    "                inner_slices.extend(range(start_slice, slices[i]))\n",
    "                start_slice = None\n",
    "\n",
    "    if start_slice is not None:  # For the case where the last block reaches the end\n",
    "        inner_slices.extend(range(start_slice, slices[-1] + 1))\n",
    "\n",
    "    return inner_slices\n",
    "\n",
    "# Function to save numpy array as .nii.gz\n",
    "def save_nii(data, filename):\n",
    "    img = nib.Nifti1Image(data, affine=np.eye(4))\n",
    "    nib.save(img, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4523e1-2cbb-4d29-80b2-43b4e382b15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1022, 512, 512)\n",
      "(1044, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# Iterate over unique patients\n",
    "patient_id = unique_patients[0]\n",
    "#for patient_id in tqdm(unique_patients[0:1]):\n",
    "eval_df = curr_df[curr_df.patient_id == patient_id]\n",
    "paths = np.array(eval_df.series_dir)\n",
    "series_names = np.array(eval_df.series_id)\n",
    "\n",
    "volumes = []\n",
    "\n",
    "# For each series in a patient, sample the volume\n",
    "for path, series_name in zip(paths, series_names):\n",
    "    vol = sample_patient_volume(path, depth_downsample_rate=None, lw_downsample_rate=None, \n",
    "                              adjust_pixel_spacing=\"no\", standardize_pixel_array=\"yes\", \n",
    "                              target_pixel_spacing=[1.0, 1.0], target_thickness=1)\n",
    "    volumes.append((series_name, vol))\n",
    "\n",
    "# Filtering based on depth criterion\n",
    "depths = [vol.shape[0] for _, vol in volumes]\n",
    "max_depth = max(depths)\n",
    "\n",
    "# Remove volumes that don't meet the depth criterion from the list\n",
    "volumes = [(series_name, vol) for series_name, vol in volumes if vol.shape[0] >= 0.4 * max_depth]\n",
    "\n",
    "# Save volumes and keep track of saved filepaths\n",
    "saved_filepaths = []\n",
    "\n",
    "for series_name, vol in volumes:\n",
    "    filename = os.path.join(vol_directory, f\"{series_name}.nii.gz\")\n",
    "    save_nii(vol, filename)\n",
    "    saved_filepaths.append(filename)\n",
    "\n",
    "# Create a data list for the Dataset using the saved_filepaths list\n",
    "data_list = [{\"image\": filepath} for filepath in saved_filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4cbf8ea0-d2cb-45c2-8549-fd2a2dde410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1022, 256, 256])\n",
      "(641, 148, 158)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 498/498 [00:01<00:00, 314.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1044, 256, 256])\n",
      "(653, 139, 147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 498/498 [00:01<00:00, 315.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "test_org_ds = Dataset(data=data_list, transform=test_org_transforms)\n",
    "test_org_loader = DataLoader(test_org_ds, batch_size=1, num_workers=0)\n",
    "\n",
    "# Initialize an empty list to store bounding box data\n",
    "bbox_data_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Loop through both filepaths (from DataLoader) and volumes\n",
    "    for (test_data, (series_name, vol)) in zip(test_org_loader, volumes):\n",
    "\n",
    "        test_inputs = test_data[\"image\"].to(seg_model_device)\n",
    "        roi_size = (96, 96, 96)\n",
    "        sw_batch_size = 4\n",
    "        curr_data = sliding_window_inference(test_inputs, roi_size, sw_batch_size, seg_model)\n",
    "        ideal_size = list(test_data['image_meta_dict']['spatial_shape'][0].cpu().detach().numpy())\n",
    "        # Process the data\n",
    "        segmented = curr_data.cpu()\n",
    "        upsampled = F.interpolate(segmented, size=[ideal_size[0], segmented.shape[3], segmented.shape[4]], mode='trilinear', align_corners=True)\n",
    "\n",
    "        merged = torch.argmax(upsampled, dim=1)# + 1  # add 1 to move the range from 0-5 to 1-6\n",
    "\n",
    "        print(merged.shape)\n",
    "\n",
    "        bounding_boxes_result_v3 = process_and_get_bounding_boxes_v6(merged)\n",
    "\n",
    "        del merged\n",
    "        del segmented\n",
    "        del upsampled\n",
    "        del curr_data\n",
    "        del ideal_size\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        extracted_vol = extract_encompassing_subregion(vol, bounding_boxes_result_v3)\n",
    "\n",
    "        print(extracted_vol.shape)\n",
    "        # Initialize the CapsuleFeatureExtractor\n",
    "        cur_features = preprocess_and_extract_torch(volume = extracted_vol, \n",
    "                                                    model = feature_extractor,\n",
    "                                                    device = feature_extractor_device,\n",
    "                                                    resize_to=(500, 224, 224), \n",
    "                                                    use_three_channel=True, \n",
    "                                                    overlap=True)\n",
    "        \n",
    "\n",
    "        base_directory = os.path.join(current_directory, \"entire_volume_features\")\n",
    "        if not os.path.exists(base_directory):\n",
    "            os.makedirs(base_directory)  # Create the 'entire_volume_features' directory\n",
    "        \n",
    "        patient_directory = os.path.join(base_directory, str(series_name))\n",
    "        if not os.path.exists(patient_directory):\n",
    "            os.makedirs(patient_directory)  # Create the subdirectory for the 'series_name'\n",
    "            \n",
    "                \n",
    "        feature_path = os.path.join(patient_directory, f\"{series_name}_features.npy\")\n",
    "        np.save(feature_path, cur_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2c350-17d9-4456-a931-2e5971ad49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_extractor = CombinedFeatureExtractor(input_channels = 3, pretrained=True, unet_feature_dim=1024, capsnet_feature_dim=1024)\n",
    "feature_extractor = feature_extractor.to(feature_extractor_device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a069fab0-9600-4283-8442-ad57143ed1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bdb8c246-381b-4502-a01c-cb4c5844a308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 503)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes_result_v3['liver']['depth_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e0f477b-20ec-4aa5-88ae-a850e9525c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 829)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes_result_v3['bowel']['depth_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6c5d1934-f29a-4328-8f33-ae8df30dfa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, 395)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes_result_v3['spleen']['depth_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ad50568-d5a2-4052-af9f-88b0f0370c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 554)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes_result_v3['kidney_left']['depth_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52ee1309-4e41-4635-b00b-da622d14622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(189, 829)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_boxes_result_v3['encompassing']['depth_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60cb48e1-0fc1-4ab0-8283-c140ba87651c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "829-189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "027d063f-81cb-4f64-8f36-b46dbd27c0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/0728tot/ATD'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934ef6e-fb3a-4f59-9ff1-5b0b82aa56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(merged[0][400])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
