{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0+95.ga4e4894d\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.0.1+cu117\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a4e4894dca25f5e87b9306abfc472805f92b69da\n",
      "MONAI __file__: /usr/local/lib/python3.8/dist-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.21.0\n",
      "scipy version: 1.10.1\n",
      "Pillow version: 10.0.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: 4.7.1\n",
      "TorchVision version: 0.15.2+cu117\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: 1.4.1\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.0.3\n",
      "einops version: 0.6.1\n",
      "transformers version: 4.32.1\n",
      "mlflow version: 2.6.0\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    DeleteItemsd,  # Remove the metadata\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "#multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpc8aczhn3\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patients: 100%|███████████████████████| 3147/3147 [00:00<00:00, 121930.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           series_dir patient_id series_id\n",
      "55    /workspace/0728tot/ATD/train_images/64194/25349      64194     25349\n",
      "56    /workspace/0728tot/ATD/train_images/64194/34232      64194     34232\n",
      "109    /workspace/0728tot/ATD/train_images/50518/1201      50518      1201\n",
      "110   /workspace/0728tot/ATD/train_images/54183/33526      54183     33526\n",
      "115   /workspace/0728tot/ATD/train_images/42008/63418      42008     63418\n",
      "...                                               ...        ...       ...\n",
      "4597  /workspace/0728tot/ATD/train_images/37551/62680      37551     62680\n",
      "4613  /workspace/0728tot/ATD/train_images/44507/21282      44507     21282\n",
      "4617  /workspace/0728tot/ATD/train_images/37436/59325      37436     59325\n",
      "4618  /workspace/0728tot/ATD/train_images/37436/50434      37436     50434\n",
      "4679     /workspace/0728tot/ATD/train_images/7642/778       7642       778\n",
      "\n",
      "[206 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the path to the train_images directory located one level up\n",
    "train_base_directory = os.path.join(current_directory, 'train_images')\n",
    "\n",
    "segmentation_directory = os.path.join(current_directory, 'segmentations')\n",
    "\n",
    "# Collect all the DICOM file paths along with patient IDs and series IDs\n",
    "dicom_data = []\n",
    "for patient_id in tqdm(os.listdir(train_base_directory), desc=\"Processing Patients\"):\n",
    "    patient_dir = os.path.join(train_base_directory, patient_id)\n",
    "    for series_id in os.listdir(patient_dir):\n",
    "        series_dir = os.path.join(patient_dir, series_id)\n",
    "        dicom_data.append({'series_dir': series_dir, 'patient_id': patient_id, 'series_id': series_id})\n",
    "\n",
    "# Create a DataFrame using the collected data\n",
    "dicom_df = pd.DataFrame(dicom_data)\n",
    "\n",
    "# Gather all the series IDs from the segmentation folder\n",
    "segmentation_series_ids = [os.path.splitext(f)[0] for f in os.listdir(segmentation_directory)]  # Assuming .nii extension\n",
    "\n",
    "# Filter the DataFrame to only include rows with series IDs that are present in the segmentation folder\n",
    "filtered_df = dicom_df[dicom_df['series_id'].isin(segmentation_series_ids)]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pydicom as dicom\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "def sample_patient_volume(folder, output_path, depth_downsample_rate = None, lw_downsample_rate = None):\n",
    "    \"\"\"\n",
    "    Standardize the pixel array from DICOM metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = sorted([int(filename.split('.')[0]) for filename in os.listdir(folder)])\n",
    "    filenames = [str(filename) + '.dcm' for filename in filenames]\n",
    "    \n",
    "    if depth_downsample_rate:\n",
    "        filenames = filenames[::depth_downsample_rate]\n",
    "\n",
    "    volume = []\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        dcm = dicom.dcmread(filepath)\n",
    "        \n",
    "        pixel_array = dcm.pixel_array\n",
    "        if dcm.PixelRepresentation == 1:\n",
    "            bit_shift = dcm.BitsAllocated - dcm.BitsStored\n",
    "            dtype = pixel_array.dtype \n",
    "            pixel_array = (pixel_array << bit_shift).astype(dtype) >> bit_shift\n",
    "\n",
    "        intercept = float(dcm.RescaleIntercept)\n",
    "        slope = float(dcm.RescaleSlope)\n",
    "        center = int(dcm.WindowCenter)\n",
    "        width = int(dcm.WindowWidth)\n",
    "        low = center - width / 2\n",
    "        high = center + width / 2\n",
    "\n",
    "        pixel_array = (pixel_array * slope) + intercept\n",
    "        pixel_array = np.clip(pixel_array, low, high)\n",
    "\n",
    "        pixel_array -= np.min(pixel_array)\n",
    "\n",
    "        pixel_array = (pixel_array / np.max(pixel_array) * 255).astype(np.int16)\n",
    "        \n",
    "        if lw_downsample_rate:\n",
    "            pixel_array = pixel_array[::lw_downsample_rate, ::lw_downsample_rate]\n",
    "        \n",
    "        volume.append(pixel_array)\n",
    "        \n",
    "    volume = np.array(volume)\n",
    "    \n",
    "    # Save the volume as .nii.gz using nibabel\n",
    "    nifti_img = nib.Nifti1Image(volume, np.eye(4))\n",
    "    nib.save(nifti_img, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 206/206 [21:03<00:00,  6.13s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(filtered_df))):\n",
    "\n",
    "    samp = str(filtered_df.series_id.iloc[i])+'.nii.gz'\n",
    "\n",
    "    output_directory = os.path.join(current_directory, 'train_seg', samp)\n",
    "\n",
    "    sample_patient_volume(filtered_df.series_dir.iloc[i], output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "volume_dir = os.path.join(current_directory, 'train_seg')\n",
    "segmentation_dir = os.path.join(current_directory, 'segmentations')\n",
    "\n",
    "# List the files in each directory\n",
    "volume_files = sorted([f for f in os.listdir(volume_dir) if f.endswith('.nii.gz')])\n",
    "segmentation_files = sorted([f for f in os.listdir(segmentation_dir) if f.endswith('.nii')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 206/206 [17:16<00:00,  5.03s/it]\n"
     ]
    }
   ],
   "source": [
    "segmentation_alt_dir = os.path.join(current_directory, 'segmentations_alt')\n",
    "\n",
    "for i in tqdm(range(len(segmentation_files))):\n",
    "    seg_file = segmentation_files[i]\n",
    "    \n",
    "    segmentation_path = os.path.join(segmentation_dir, seg_file)\n",
    "    \n",
    "    segmentation_nii = nib.load(segmentation_path)\n",
    "    \n",
    "    segmentation_data = segmentation_nii.get_fdata()\n",
    "    \n",
    "    segmentation_data = np.transpose(segmentation_data, (2, 0, 1))\n",
    "    \n",
    "    segmentation_data = np.rot90(np.swapaxes(segmentation_data, 1, 2), 2)\n",
    "\n",
    "    nifti_img = nib.Nifti1Image(segmentation_data, np.eye(4))\n",
    "\n",
    "    new_segmentation_path = os.path.join(segmentation_alt_dir, seg_file)\n",
    "    nib.save(nifti_img, new_segmentation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directories\n",
    "volume_dir = os.path.join(current_directory, 'train_seg')\n",
    "segmentation_alt_dir = os.path.join(current_directory, 'segmentations_alt')\n",
    "\n",
    "# List the files in each directory\n",
    "volume_files = sorted([f for f in os.listdir(volume_dir) if f.endswith('.nii.gz')])\n",
    "segmentation_alt_files = sorted([f for f in os.listdir(segmentation_alt_dir) if f.endswith('.nii')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df = pd.read_csv('bbox_data.csv')\n",
    "bbox_series = []\n",
    "\n",
    "for i in range(len(bbox_df)):\n",
    "    bbox_series.append(bbox_df.series_id.iloc[i].split('/')[-1])\n",
    "\n",
    "bbox_df['series_index'] = bbox_series\n",
    "\n",
    "bbox_df = bbox_df[bbox_df.series_index.isin(filtered_df.series_id)]\n",
    "bbox_df = bbox_df.sort_values(by='series_index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "# Remove unwanted characters and split string into list of strings\n",
    "data_list = bbox_df['bbox_x'][i].replace(\"[\", \"\").replace(\"]\", \"\").split(\"\\n\")\n",
    "bounding_boxes = [list(map(int, item.strip().split())) for item in data_list]\n",
    "bounding_boxes = bounding_boxes[int(len(bounding_boxes)*0.05):int(len(bounding_boxes)*.95)]\n",
    "# bounding_box = bounding_boxes[bbox_df['max_area_index'][i]] max area\n",
    "\n",
    "mean_bbox = np.mean(bounding_boxes, axis=0)\n",
    "distances = np.linalg.norm(bounding_boxes - mean_bbox, axis=1)\n",
    "closest_index = np.argmin(distances)\n",
    "\n",
    "bounding_box = bounding_boxes[closest_index]\n",
    "\n",
    "vol_file = volume_files[i]\n",
    "seg_file = segmentation_alt_files[i]\n",
    "\n",
    "volume_path = os.path.join(volume_dir, vol_file)\n",
    "\n",
    "segmentation_alt_path = os.path.join(segmentation_alt_dir, seg_file)\n",
    "\n",
    "volume_nii = nib.load(volume_path)\n",
    "segmentation_alt_nii = nib.load(segmentation_alt_path)\n",
    "\n",
    "volume_data = volume_nii.get_fdata()\n",
    "segmentation_alt_data = segmentation_alt_nii.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 206/206 [01:57<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "bad_cases = []\n",
    "for i in tqdm(range(len(volume_files))):\n",
    "    vol_file = volume_files[i]\n",
    "    seg_file = segmentation_alt_files[i]\n",
    "    \n",
    "    volume_path = os.path.join(volume_dir, vol_file)\n",
    "    \n",
    "    segmentation_alt_path = os.path.join(segmentation_alt_dir, seg_file)\n",
    "    \n",
    "    volume_nii = nib.load(volume_path)\n",
    "    segmentation_alt_nii = nib.load(segmentation_alt_path)\n",
    "    \n",
    "    volume_data = volume_nii.get_fdata()\n",
    "    segmentation_alt_data = segmentation_alt_nii.get_fdata()\n",
    "    \n",
    "    if volume_data.shape!=segmentation_alt_data.shape:\n",
    "        bad_cases.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10109.nii.gz'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_files[bad_cases[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
