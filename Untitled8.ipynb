{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a23e85-c011-4b14-85a0-d13bc5a61072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "\n",
    "    # When using GPU\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c614abc6-fb08-4d5f-861d-ce6b76f28bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing .npy files: 100%|█████████████████████████████████████████████████████████████████████████| 4579/4579 [00:00<00:00, 6357.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_npy_data(base_directory):\n",
    "    \"\"\"\n",
    "    Traverse the directory structure and collect paths to .npy files.\n",
    "    \n",
    "    Args:\n",
    "    - base_directory (str): Root directory containing .npy files.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Contains .npy file path, series_id, and numpy data for each .npy file.\n",
    "    \"\"\"\n",
    "    # List to collect data\n",
    "    npy_data = []\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in tqdm(os.listdir(base_directory), desc=\"Processing .npy files\"):\n",
    "        # Check if the current file is a .npy file\n",
    "        if file_name.endswith('.npy'):\n",
    "            series_id = int(file_name.split(\"_\")[0])  # Extracting series_id from the filename\n",
    "            file_path = os.path.join(base_directory, file_name)\n",
    "            \n",
    "            # Load numpy data\n",
    "            npy_array = np.load(file_path)\n",
    "            \n",
    "            # Append details to the list\n",
    "            npy_data.append({\n",
    "                'npy_path': file_path,\n",
    "                'series_id': series_id,\n",
    "                'npy_data': npy_array\n",
    "            })\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    npy_df = pd.DataFrame(npy_data)\n",
    "    \n",
    "    return npy_df\n",
    "\n",
    "# Example usage\n",
    "base_directory = 'volume_info'  # Replace with your directory path\n",
    "npy_df = collect_npy_data(base_directory)\n",
    "npy_df=npy_df.sort_values(by='series_id',axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998ede3a-7d1b-411f-8bbf-28e4755b8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_last_indices(array, value):\n",
    "    \"\"\"\n",
    "    Get the first and last index of a value in a numpy array.\n",
    "\n",
    "    Args:\n",
    "    - array (numpy.ndarray): The input numpy array.\n",
    "    - value (int/float): The value to search for.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (first index, last index) normalized by dividing by the total length of the array.\n",
    "    \"\"\"\n",
    "    indices = np.where(array == value)[0]\n",
    "    if indices.size == 0:\n",
    "        return [0, 0]\n",
    "    first_index = indices[0] / len(array)\n",
    "    last_index = indices[-1] / len(array)\n",
    "    return [first_index, last_index]\n",
    "\n",
    "def trim_zero_rows(matrix):\n",
    "    \"\"\"\n",
    "    Remove rows from the top and bottom of the matrix that contain only zeros.\n",
    "    Stop when a row with a non-zero entry is encountered from both directions.\n",
    "\n",
    "    Args:\n",
    "    - matrix (numpy.ndarray): 2D numpy array\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Trimmed matrix\n",
    "    \"\"\"\n",
    "    # Find the index of the first row from the top that contains a non-zero entry\n",
    "    first_non_zero_row = np.argmax(np.any(matrix != 0, axis=1))\n",
    "    \n",
    "    # Find the index of the first row from the bottom that contains a non-zero entry\n",
    "    last_non_zero_row = matrix.shape[0] - 1 - np.argmax(np.any(matrix[::-1] != 0, axis=1))\n",
    "    \n",
    "    # Slice the matrix between these two rows\n",
    "    trimmed_matrix = matrix[first_non_zero_row:last_non_zero_row+1]\n",
    "    \n",
    "    return trimmed_matrix\n",
    "\n",
    "\n",
    "# ['liver', 'kidneys', 'spleen', 'bowel']\n",
    "\n",
    "liver_range_list = []\n",
    "kidneys_range_list = []\n",
    "spleen_range_list = []\n",
    "bowel_range_list = []\n",
    "\n",
    "bad_liver_indices = []\n",
    "bad_kidneys_indices = []\n",
    "bad_spleen_indices = []\n",
    "bad_bowel_indices = []\n",
    "\n",
    "series_depth_lengths = []\n",
    "\n",
    "for i in range(len(npy_df.npy_data)):\n",
    "    temp = npy_df.npy_data.iloc[i]\n",
    "    temp = trim_zero_rows(temp)\n",
    "\n",
    "    series_depth_lengths.append(len(temp[:,0]))\n",
    "    \n",
    "    liver_range = get_first_last_indices(temp[:,0], 1)\n",
    "    kidneys_range = get_first_last_indices(temp[:,1], 1)\n",
    "    spleen_range = get_first_last_indices(temp[:,2], 1)\n",
    "    bowel_range = get_first_last_indices(temp[:,3], 1)\n",
    "    \n",
    "    # Check if the range of each organ is (0,0) and append the index `i` to the corresponding list\n",
    "    if liver_range == [0,0]:\n",
    "        bad_liver_indices.append(i)\n",
    "    if kidneys_range == [0,0]:\n",
    "        bad_kidneys_indices.append(i)\n",
    "    if spleen_range == [0,0]:\n",
    "        bad_spleen_indices.append(i)\n",
    "    if bowel_range == [0,0]:\n",
    "        bad_bowel_indices.append(i)\n",
    "    \n",
    "    # Append the range values to the respective lists\n",
    "    liver_range_list.append(liver_range)\n",
    "    kidneys_range_list.append(kidneys_range)\n",
    "    spleen_range_list.append(spleen_range)\n",
    "    bowel_range_list.append(bowel_range)\n",
    "\n",
    "bad_cases = np.unique(bad_liver_indices + bad_kidneys_indices + bad_spleen_indices + bad_bowel_indices)\n",
    "\n",
    "liver_range_list = np.array(liver_range_list)\n",
    "kidneys_range_list = np.array(kidneys_range_list)\n",
    "spleen_range_list = np.array(spleen_range_list)\n",
    "bowel_range_list = np.array(bowel_range_list)\n",
    "\n",
    "mask = np.ones(liver_range_list.shape[0], dtype=bool)\n",
    "mask[bad_cases] = False\n",
    "\n",
    "original_liver_range_list = liver_range_list#[bad_cases, :]\n",
    "original_kidneys_range_list = kidneys_range_list#[bad_cases, :]\n",
    "original_spleen_range_list = spleen_range_list#[bad_cases, :]\n",
    "original_bowel_range_list = bowel_range_list#[bad_cases,:]\n",
    "\n",
    "liver_range_list = liver_range_list[mask, :]\n",
    "kidneys_range_list = kidneys_range_list[mask, :]\n",
    "spleen_range_list = spleen_range_list[mask, :]\n",
    "bowel_range_list = bowel_range_list[mask,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88831cb-af68-4ff2-9989-3d335e6b3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the bad organs for each unique index\n",
    "bad_organs_dict = {}\n",
    "\n",
    "for idx in bad_cases:\n",
    "    is_bad_liver = int(idx in bad_liver_indices)\n",
    "    is_bad_kidneys = int(idx in bad_kidneys_indices)\n",
    "    is_bad_spleen = int(idx in bad_spleen_indices)\n",
    "    is_bad_bowel = int(idx in bad_bowel_indices)\n",
    "    \n",
    "    # Store the tuple (or list) for this index\n",
    "    bad_organs_dict[idx] = (is_bad_liver, is_bad_kidneys, is_bad_spleen, is_bad_bowel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b514bf-8d10-4a8a-bd58-a5b1d43167be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as model_predicting_kidneys_using_liver_bowel.joblib\n",
      "Using ['liver', 'bowel'] to predict kidneys - Mean Squared Error: 0.008121130851737997\n",
      "Model saved as model_predicting_kidneys_using_liver_spleen_bowel.joblib\n",
      "Using ['liver', 'spleen', 'bowel'] to predict kidneys - Mean Squared Error: 0.00597411826090619\n",
      "Model saved as model_predicting_spleen_using_liver_bowel.joblib\n",
      "Using ['liver', 'bowel'] to predict spleen - Mean Squared Error: 0.008771595875657612\n",
      "Model saved as model_predicting_spleen_using_liver_kidneys_bowel.joblib\n",
      "Using ['liver', 'kidneys', 'bowel'] to predict spleen - Mean Squared Error: 0.006961597896065943\n",
      "Model saved as model_predicting_bowel_using_liver_kidneys_spleen.joblib\n",
      "Using ['liver', 'kidneys', 'spleen'] to predict bowel - Mean Squared Error: 0.003018561741056083\n",
      "Model saved as model_predicting_bowel_using_liver_kidneys.joblib\n",
      "Using ['liver', 'kidneys'] to predict bowel - Mean Squared Error: 0.0034926491443415964\n",
      "Model saved as model_predicting_bowel_using_liver.joblib\n",
      "Using ['liver'] to predict bowel - Mean Squared Error: 0.005451986822721287\n",
      "Model saved as model_predicting_kidneys_using_liver.joblib\n",
      "Using ['liver'] to predict kidneys - Mean Squared Error: 0.016946467793744244\n",
      "Model saved as model_predicting_spleen_using_liver.joblib\n",
      "Using ['liver'] to predict spleen - Mean Squared Error: 0.012242793143504523\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_datasets(input_organs, target_organ):\n",
    "    # Convert the organ names to their respective lists\n",
    "    organ_dict = {\n",
    "        \"liver\": liver_range_list,\n",
    "        \"kidneys\": kidneys_range_list,\n",
    "        \"spleen\": spleen_range_list,\n",
    "        \"bowel\": bowel_range_list\n",
    "    }\n",
    "    \n",
    "    # Create input and target datasets\n",
    "    X = np.hstack([organ_dict[organ] for organ in input_organs])\n",
    "    y = organ_dict[target_organ]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Define the combinations\n",
    "combinations = [\n",
    "    ([\"liver\", \"bowel\"], \"kidneys\"),\n",
    "    ([\"liver\", \"spleen\", \"bowel\"], \"kidneys\"),\n",
    "    ([\"liver\", \"bowel\"], \"spleen\"),\n",
    "    ([\"liver\", \"kidneys\", \"bowel\"], \"spleen\"),\n",
    "    ([\"liver\", \"kidneys\", \"spleen\"], \"bowel\"),  \n",
    "    ([\"liver\", \"kidneys\"], \"bowel\"),  \n",
    "    ([\"liver\"], \"bowel\"),  \n",
    "    ([\"liver\"], \"kidneys\"),\n",
    "    ([\"liver\"], \"spleen\")\n",
    "]\n",
    "\n",
    "# For each combination\n",
    "for input_organs, target_organ in combinations:\n",
    "    X_train, X_test, y_train, y_test = generate_datasets(input_organs, target_organ)\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f\"model_predicting_{target_organ}_using_{'_'.join(input_organs)}.joblib\"\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Model saved as {model_filename}\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Using {input_organs} to predict {target_organ} - Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c40cd2-fddd-4aee-a11d-8b21dcfb8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "models_dict = {}\n",
    "\n",
    "# Directory where your models are saved\n",
    "models_directory = os.getcwd()\n",
    "\n",
    "for input_organs, target_organ in combinations:\n",
    "    model_filename = f\"model_predicting_{target_organ}_using_{'_'.join(input_organs)}.joblib\"\n",
    "    model_path = os.path.join(models_directory, model_filename)\n",
    "    \n",
    "    # Load the model and save to the dictionary\n",
    "    model = joblib.load(model_path)\n",
    "    key = (tuple(input_organs), target_organ)\n",
    "    models_dict[key] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ef8e5b-b17a-431a-be94-77a45adbfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_index(index, available_organs):\n",
    "    \"\"\"\n",
    "    Extracts and returns the data corresponding to the given index for the specified organs.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    if \"liver\" in available_organs:\n",
    "        data_list.append(original_liver_range_list[index])\n",
    "    if \"kidneys\" in available_organs:\n",
    "        data_list.append(original_kidneys_range_list[index])\n",
    "    if \"spleen\" in available_organs:\n",
    "        data_list.append(original_spleen_range_list[index])\n",
    "    if \"bowel\" in available_organs:\n",
    "        data_list.append(original_bowel_range_list[index])\n",
    "    \n",
    "    # Combine the data as required\n",
    "    combined_data = np.hstack(data_list)\n",
    "    \n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f771f935-fa8d-43ef-9ecc-538e43869ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions\n",
    "predictions = {}\n",
    "\n",
    "# Iterate through the bad_organs_dict\n",
    "for index, bad_organs in bad_organs_dict.items():\n",
    "    # Convert bad_organs to a list of organ names\n",
    "    available_organs = []\n",
    "    for i, organ in enumerate(['liver', 'kidneys', 'spleen', 'bowel']):\n",
    "        if bad_organs[i] == 0:  # If the organ is available\n",
    "            available_organs.append(organ)\n",
    "    \n",
    "    # For each organ, if it's missing, try to predict it using the available organs\n",
    "    for i, organ in enumerate(['liver', 'kidneys', 'spleen', 'bowel']):\n",
    "        if bad_organs[i] == 1:  # If the organ is missing\n",
    "            key = (tuple(available_organs), organ)\n",
    "            model = models_dict.get(key)\n",
    "            if model:\n",
    "                data = get_data_for_index(index, available_organs)\n",
    "                prediction = model.predict([data])\n",
    "                predictions[(index, organ)] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91042a9-cb5c-4f7d-a731-5e4b78bdb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming predictions_dict is the dictionary where you've saved the predictions\n",
    "# For example: predictions_dict = {(11, 'kidneys'): array([0.71556257, 0.94957948]), ...}\n",
    "for (index, organ), prediction in predictions.items():\n",
    "    if organ == 'kidneys':\n",
    "        original_kidneys_range_list[index] = prediction\n",
    "    elif organ == 'liver':\n",
    "        original_liver_range_list[index] = prediction\n",
    "    elif organ == 'spleen':\n",
    "        original_spleen_range_list[index] = prediction\n",
    "    elif organ == 'bowel':\n",
    "        original_bowel_range_list[index] = prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3187aa0e-8298-4bc0-975e-0f642e09995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "definite_kidneys_range_list = original_kidneys_range_list.T*series_depth_lengths\n",
    "definite_kidneys_range_list = definite_kidneys_range_list.T\n",
    "\n",
    "definite_liver_range_list = original_liver_range_list.T*series_depth_lengths\n",
    "definite_liver_range_list = definite_liver_range_list.T\n",
    "\n",
    "definite_spleen_range_list = original_spleen_range_list.T*series_depth_lengths\n",
    "definite_spleen_range_list = definite_spleen_range_list.T\n",
    "\n",
    "definite_bowel_range_list = original_bowel_range_list.T*series_depth_lengths\n",
    "definite_bowel_range_list = definite_bowel_range_list.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d553dd2b-5bf2-40cb-8a0f-018bb6a1d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52a4b86e-b254-41cc-881e-83c5d18a6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_channel(channel):\n",
    "    # 1. Thresholding\n",
    "    min_HU = 55  # You'll need to adjust these values based on your dataset\n",
    "    max_HU = 250\n",
    "    _, thresh = cv2.threshold(channel, min_HU, max_HU, cv2.THRESH_BINARY)\n",
    "\n",
    "    thresh = cv2.medianBlur(thresh, 5)\n",
    "    # 2. Morphological Operations\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    dilated = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(dilated, 8, cv2.CV_32S)\n",
    "    areas = stats[:,-1]\n",
    "    \n",
    "    # Exclude the background label and get the two largest areas\n",
    "    largest_indices = np.argsort(areas[1:])[-2:] + 1\n",
    "    \n",
    "    connected = np.zeros_like(dilated, np.uint8)\n",
    "    for idx in largest_indices:\n",
    "        connected[labels == idx] = 255\n",
    "    # 5. Masking\n",
    "    result_channel = cv2.bitwise_and(channel, connected)\n",
    "    result_channel[result_channel > 210] = 0\n",
    "\n",
    "    return result_channel\n",
    "\n",
    "def process_image(image_path):\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Split into RGB channels\n",
    "    r, g, b = cv2.split(image)\n",
    "    \n",
    "    # Process each channel\n",
    "    r_processed = process_channel(r)\n",
    "    g_processed = process_channel(g)\n",
    "    b_processed = process_channel(b)\n",
    "    \n",
    "    # Merge processed channels\n",
    "    processed_image = cv2.merge([r_processed, g_processed, b_processed])\n",
    "\n",
    "    return processed_image\n",
    "\n",
    "def batch_process_image(filepaths, output_dir):\n",
    "    \"\"\"\n",
    "    Process and save images to the specified output directory.\n",
    "    \n",
    "    Args:\n",
    "    - filepaths (list): List of file paths to process.\n",
    "    - output_dir (str): Directory where the processed images will be saved.\n",
    "    \"\"\"\n",
    "    for filepath in filepaths:\n",
    "        processed_image = process_image(filepath)\n",
    "        \n",
    "        # Construct output path\n",
    "        output_path = os.path.join(output_dir, os.path.basename(filepath))\n",
    "        \n",
    "        # Save the processed image\n",
    "        cv2.imwrite(output_path, processed_image)\n",
    "        print(f\"Processed image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc693db1-f66f-4a30-970f-61a9622270e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 4579/4579 [00:01<00:00, 3687.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_jpg_data(base_directory):\n",
    "    \"\"\"\n",
    "    Traverse the directory structure and collect paths to .jpg files.\n",
    "    \n",
    "    Args:\n",
    "    - base_directory (str): Root directory containing subdirectories of .jpg files.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Contains .jpg file path, series_id, and image data for each .jpg file.\n",
    "    \"\"\"\n",
    "    # List to collect data\n",
    "    jpg_data = []\n",
    "\n",
    "    # Iterate through all subdirectories in the base directory\n",
    "    for sub_dir in tqdm(os.listdir(base_directory)):\n",
    "        sub_dir_path = os.path.join(base_directory, sub_dir)\n",
    "        \n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            series_id = int(sub_dir)  # Assuming the subdirectory name is the series_id\n",
    "            \n",
    "            # Iterate through all files in the subdirectory\n",
    "            for file_name in os.listdir(sub_dir_path):\n",
    "                # Check if the current file is a .jpg file\n",
    "                if file_name.endswith('.jpg'):\n",
    "                    file_path = os.path.join(sub_dir_path, file_name)\n",
    "\n",
    "                    name = file_name.split('.')[0]\n",
    "                    # Append details to the list\n",
    "                    jpg_data.append({\n",
    "                        'jpg_path': file_path,\n",
    "                        'series_id': series_id,\n",
    "                        'file_name': name\n",
    "                    })\n",
    "\n",
    "    # Convert the list into a DataFrame\n",
    "    jpg_df = pd.DataFrame(jpg_data)\n",
    "    \n",
    "    return jpg_df\n",
    "\n",
    "# Example usage\n",
    "base_directory = 'volume_images'  # Replace with your directory path\n",
    "jpg_df = collect_jpg_data(base_directory)\n",
    "jpg_df=jpg_df.sort_values(by='series_id',axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "936d857f-9e5a-4181-b426-a695a9ad2235",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'series_image_split' \n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e92f23-1e51-4c32-98bc-d51ab03025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_save_images_from_paths(series_id, depth_range, organ, filepaths, filenames, base_path):\n",
    "    \"\"\"\n",
    "    Extracts and saves images based on depth ranges for each organ.\n",
    "    \n",
    "    Args:\n",
    "    - series_id (str): The ID of the series.\n",
    "    - depth_ranges (list of tuples): The depth ranges of the organ in the format [(start1, end1), (start2, end2), ...].\n",
    "    - organ (str): The name of the organ (e.g., \"liver\", \"kidneys\").\n",
    "    - filepaths (list of str): List of paths to the image files.\n",
    "    - base_path (str): Base directory to save the extracted images.\n",
    "    \"\"\"\n",
    "    series_path = os.path.join(base_path, series_id)\n",
    "    organ_path = os.path.join(series_path, organ)\n",
    "    \n",
    "    if not os.path.exists(organ_path):\n",
    "        os.makedirs(organ_path)\n",
    "\n",
    "    start, end = depth_range\n",
    "    if organ == 'bowel' and end > 250:\n",
    "        start = start + 30\n",
    "        \n",
    "    for i in range(len(filepaths)):\n",
    "        filepath = filepaths[i]\n",
    "        filename = filenames[i]\n",
    "        # Extracting the filename without extension            \n",
    "        # Check if the filename is a number and within the specified depth range\n",
    "        if filename.isdigit() and start <= int(filename) <= end:\n",
    "\n",
    "            img = Image.open(filepath)\n",
    "            save_path = os.path.join(organ_path, f\"{filename}.jpg\")\n",
    "            img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "21157df7-307a-44f6-b57b-644cbdc9a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 4579/4579 [09:07<00:00,  8.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(series_list))):\n",
    "    series_list = np.unique(jpg_df.series_id)\n",
    "    curr_id = series_list[i]\n",
    "    jpg_path_list = np.array(jpg_df[jpg_df.series_id==curr_id].jpg_path)\n",
    "    jpg_name_list = np.array(jpg_df[jpg_df.series_id==curr_id].file_name)\n",
    "    \n",
    "    extract_and_save_images_from_paths(str(curr_id), definite_kidneys_range_list[i], 'kidneys', jpg_path_list, jpg_name_list, base_path)\n",
    "    extract_and_save_images_from_paths(str(curr_id), definite_bowel_range_list[i], 'bowel', jpg_path_list, jpg_name_list, base_path)\n",
    "    extract_and_save_images_from_paths(str(curr_id), definite_liver_range_list[i], 'liver', jpg_path_list, jpg_name_list, base_path)\n",
    "    extract_and_save_images_from_paths(str(curr_id), definite_spleen_range_list[i], 'spleen', jpg_path_list, jpg_name_list, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "c882a978-13ef-4c13-99f9-c18b7f7be899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_folder = \"series_image_split\"  # Replace with the actual path to your base folder\n",
    "\n",
    "# List of organs\n",
    "organs = [\"kidneys\", \"liver\", \"spleen\", \"bowel\"]\n",
    "\n",
    "# Dictionary to store the counts\n",
    "distribution = {}\n",
    "\n",
    "# Iterate through each series_id folder\n",
    "for series_id in os.listdir(base_folder):\n",
    "    series_path = os.path.join(base_folder, series_id)\n",
    "    \n",
    "    # Ensure it's a directory and not a file\n",
    "    if os.path.isdir(series_path):\n",
    "        distribution[series_id] = {}\n",
    "        \n",
    "        # Iterate through each organ folder under the current series_id folder\n",
    "        for organ in organs:\n",
    "            organ_path = os.path.join(series_path, organ)\n",
    "            \n",
    "            # Check if the organ directory exists\n",
    "            if os.path.exists(organ_path):\n",
    "                # Count the number of image files in the organ directory\n",
    "                count = sum([1 for file in os.listdir(organ_path) if file.endswith('.jpg') or file.endswith('.png')])  # Assuming images are in .jpg or .png format\n",
    "                distribution[series_id][organ] = count\n",
    "            else:\n",
    "                distribution[series_id][organ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "fe44ecfd-ba3d-4c59-a238-b5371c2f97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you already have the 'distribution' dictionary from the previous code snippet\n",
    "\n",
    "# Extract data for plotting\n",
    "series_ids = list(distribution.keys())\n",
    "kidneys_counts = [distribution[sid]['kidneys'] for sid in series_ids]\n",
    "liver_counts = [distribution[sid]['liver'] for sid in series_ids]\n",
    "spleen_counts = [distribution[sid]['spleen'] for sid in series_ids]\n",
    "bowel_counts = [distribution[sid]['bowel'] for sid in series_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "3a50ebb6-3164-404d-aa69-4a87ad114d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 inputs *15 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "ae12e1b6-51d2-40c3-96e6-a0ebc09c0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.992138021402052"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(kidneys_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "02f5db50-a664-4b9b-a16b-142ea701cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.748635073160077"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(liver_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "b83d2b7c-fa86-400a-bb03-ef1631e5bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.555143044332823"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(spleen_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "b8ff3357-c647-4abe-b18e-e3227cde5c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.67613015942346"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(bowel_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65341c0f-4e8e-4cce-a1b4-263f79a0324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create customdataset that loads in all images for a particular volume, particular series_id\n",
    "# resize/crop the images to 224x224 and shrink/increase them to 15 slices. \n",
    "# crop by reducing the width, so it conforms more like a 1.15 ratio between width and height\n",
    "# Do it for RGB channels\n",
    "\n",
    "# Afterwards assign them labels based on their injury status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9fc99e-d40b-4bc0-8178-83e89fa78bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "# def set_seed(seed_value=42):\n",
    "#     \"\"\"Set seed for reproducibility.\"\"\"\n",
    "#     random.seed(seed_value)\n",
    "#     np.random.seed(seed_value)\n",
    "#     torch.manual_seed(seed_value)\n",
    "\n",
    "#     # When using GPU\n",
    "#     if torch.cuda.is_available(): \n",
    "#         torch.cuda.manual_seed(seed_value)\n",
    "#         torch.cuda.manual_seed_all(seed_value)\n",
    "#         # torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd960e4-bf41-4763-8c83-26eb6eeaff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "def collect_image_paths(base_folder):\n",
    "    \"\"\"\n",
    "    Traverse the directory structure under base_folder and collect paths to all image files.\n",
    "    \n",
    "    Args:\n",
    "    - base_folder (str): The main directory containing all series_id subfolders.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: Contains columns 'series_id', 'organ', 'image_path', and 'image_name'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each series_id in the main folder\n",
    "    for series_id in os.listdir(base_folder):\n",
    "        series_path = os.path.join(base_folder, series_id)\n",
    "        \n",
    "        # Check if it's a directory and not a file\n",
    "        if os.path.isdir(series_path):\n",
    "            \n",
    "            # Iterate through each organ folder under the current series_id\n",
    "            for organ in ['bowel', 'kidneys', 'liver', 'spleen']:\n",
    "                organ_path = os.path.join(series_path, organ)\n",
    "                \n",
    "                # Collect all .jpg image paths under the organ directory\n",
    "                organ_images = [os.path.join(organ_path, fname) for fname in os.listdir(organ_path) if fname.endswith('.jpg')]\n",
    "                \n",
    "                for image_path in organ_images:\n",
    "                    image_name = os.path.basename(image_path).replace('.jpg', '')  # Extract filename without extension\n",
    "                    data.append({\n",
    "                        'series_id': series_id,\n",
    "                        'organ': organ,\n",
    "                        'image_path': image_path,\n",
    "                        'image_name': image_name\n",
    "                    })\n",
    "                    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['series_id'] = df['series_id'].astype(int)\n",
    "    df['image_name'] = df['image_name'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "base_folder = 'series_image_split'\n",
    "all_image_paths = collect_image_paths(base_folder)\n",
    "all_image_paths = all_image_paths.sort_values(by=['series_id','image_name'],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82844488-0c65-45f7-adfb-e2816ba79474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_info = pd.read_csv('train.csv')\n",
    "train_info.patient_id = train_info.patient_id.astype(str)\n",
    "\n",
    "mapping_df = pd.read_csv('patient_series_mapping.csv')\n",
    "mapping_df = mapping_df.drop('Unnamed: 0',axis=1)\n",
    "mapping_df = mapping_df[['patient_id','series_id']]\n",
    "mapping_df['patient_id'] = mapping_df['patient_id'].astype(int)\n",
    "mapping_df['series_id'] = mapping_df['series_id'].astype(int)\n",
    "\n",
    "merged_df = all_image_paths.merge(mapping_df[['series_id', 'patient_id']], on='series_id', how='left')\n",
    "\n",
    "y_original_format = train_info.drop(['bowel_healthy','extravasation_healthy','any_injury'], axis=1)\n",
    "y_original_format['patient_id'] = y_original_format['patient_id'].astype(int)\n",
    "\n",
    "merged_df = merged_df.merge(y_original_format[['patient_id','bowel_injury', 'extravasation_injury', 'kidney_healthy',\n",
    "       'kidney_low', 'kidney_high', 'liver_healthy', 'liver_low', 'liver_high',\n",
    "       'spleen_healthy', 'spleen_low', 'spleen_high']], on='patient_id', how='left')\n",
    "\n",
    "y_original_format = train_info.drop(['patient_id','bowel_healthy','extravasation_healthy','any_injury'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfbf2b43-fb43-4a4e-82fc-4b436e2fa70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OrganTrainDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.series_ids = self.dataframe['series_id'].unique()\n",
    "        self.organs = ['liver', 'spleen', 'kidneys', 'bowel']\n",
    "        \n",
    "        # Define columns that contain labels\n",
    "        self.label_columns = ['bowel_injury', 'extravasation_injury', 'kidney_healthy', \n",
    "                              'kidney_low', 'kidney_high', 'liver_healthy', 'liver_low', \n",
    "                              'liver_high', 'spleen_healthy', 'spleen_low', 'spleen_high']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        series_id = self.series_ids[idx]\n",
    "        curr_df = self.dataframe[self.dataframe['series_id'] == series_id]\n",
    "\n",
    "        # 4 organs * 15 depth \n",
    "        images = []\n",
    "        for org in self.organs:\n",
    "            org_df = curr_df[curr_df.organ == org]\n",
    "            org_path = np.array(org_df.image_path)\n",
    "            org_images = []  # Store images for the current organ\n",
    "            for path in org_path:\n",
    "                image = cv2.imread(path)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                if self.transform:\n",
    "                    image = self.transform(image=image)['image']\n",
    "                image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n",
    "                org_images.append(image)\n",
    "            \n",
    "            # Stack images for the current organ and resize depth to 15\n",
    "            org_images = np.stack(org_images, 1)\n",
    "            org_images = torch.tensor(org_images)\n",
    "            resized_org_images = F.interpolate(org_images.unsqueeze(0), size=(15, 224, 224), mode='trilinear', align_corners=True).squeeze(0)\n",
    "            # print(resized_org_images.shape)\n",
    "            images.append(resized_org_images)\n",
    "    \n",
    "        # Stack all organs' images together\n",
    "        images = torch.cat(images, 1)\n",
    "        images = images.transpose(1, 0)\n",
    "        \n",
    "        # print(images.shape)\n",
    "        \n",
    "        # Extract labels for the given series_id\n",
    "        labels = curr_df.iloc[0][self.label_columns].values.astype(np.float32)\n",
    "        \n",
    "        return images, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bea3aed-2305-4c80-8a53-b329489cc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "573a7c66-93a9-4d9b-b6fe-e9ca2a40d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "\n",
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.RandomBrightnessContrast(contrast_limit=0.2, brightness_limit=0, p=1.0),\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, \n",
    "                                    rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, p=1.0),\n",
    "    albumentations.CoarseDropout(max_holes=2, max_height=int(0.4*image_size), \n",
    "                          max_width=int(0.4*image_size), fill_value=0, always_apply=True, p=1.0),\n",
    "])\n",
    "\n",
    "\n",
    "transforms_valid = albumentations.Compose([\n",
    "    albumentations.Resize(image_size, image_size),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb590ab-0933-4538-a5dd-0adae879be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_organs = 4  # Number of organs\n",
    "n_slice_per_c = 15\n",
    "bs = 1\n",
    "out_dim = 5\n",
    "pretrained = True\n",
    "drop_path_rate = 0.1\n",
    "drop_rate_last = 0.3\n",
    "drop_rate = 0.1\n",
    "backbone = 'convnext_nano'\n",
    "use_amp = True\n",
    "\n",
    "in_chans = 3\n",
    "\n",
    "class TimmModelType2(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelType2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=out_dim,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):  # (bs, n_slice_per_c * n_organs * in_chans, H, W)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * n_organs, in_chans, image_size, image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * n_organs, -1)\n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = feat1.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        feat2, _ = self.lstm2(feat)\n",
    "        return self.head(feat1), self.head2(feat2[:, 0])\n",
    "\n",
    "class TimmModelWithDualLSTM(nn.Module):\n",
    "    def __init__(self, backbone, pretrained=False):\n",
    "        super(TimmModelWithDualLSTM, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=in_chans,\n",
    "            num_classes=1,\n",
    "            features_only=False,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        # Determine the dimension of the features from the encoder\n",
    "        if 'efficient' in backbone:\n",
    "            hdim = self.encoder.conv_head.out_channels\n",
    "            self.encoder.classifier = nn.Identity()\n",
    "        elif 'convnext' in backbone:\n",
    "            hdim = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Identity()\n",
    "\n",
    "        # Binary classification LSTM\n",
    "        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 2)  # 2 binary labels\n",
    "        )\n",
    "\n",
    "        # Multiclass classification LSTM\n",
    "        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=drop_rate, bidirectional=True, batch_first=True)\n",
    "        self.head2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(drop_rate_last),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(256, 9)  # 9 multiclass labels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (bs, n_slice_per_c * n_organs * in_chans, H, W)\n",
    "        bs = x.shape[0]\n",
    "        x = x.view(bs * n_slice_per_c * n_organs, in_chans, image_size, image_size)\n",
    "        feat = self.encoder(x)\n",
    "        feat = feat.view(bs, n_slice_per_c * n_organs, -1)\n",
    "        \n",
    "        feat1, _ = self.lstm(feat)\n",
    "        feat1 = torch.mean(feat1, dim=1)\n",
    "        #feat1 = feat1.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        binary_out = self.head(feat1)\n",
    "              \n",
    "        feat2, _ = self.lstm2(feat)\n",
    "        feat2 = torch.mean(feat2, dim=1)\n",
    "        #feat2 = feat2.contiguous().view(bs * n_slice_per_c * n_organs, 512)\n",
    "        multiclass_out = self.head2(feat2)\n",
    "        \n",
    "        return binary_out, multiclass_out#[0,:], multiclass_out[0,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74595989-2977-4feb-abd5-006e542efba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = [10,1]\n",
    "\n",
    "def train_func(model, loader_train, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    binary_loss_list = []\n",
    "    multiclass_loss_list = []\n",
    "    bar = tqdm(loader_train)\n",
    "    \n",
    "    for images, targets in bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(model_device)\n",
    "        targets = targets.to(model_device)\n",
    "\n",
    "        do_mixup = False\n",
    "        if random.random() < p_mixup:\n",
    "            do_mixup = True\n",
    "            images, targets, targets_mix, lam = mixup(images, targets)\n",
    "\n",
    "        with amp.autocast():\n",
    "            logits_binary, logits_multiclass = model(images)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            loss_binary = binary_criterion(logits_binary, targets)\n",
    "            loss_multiclass = multiclass_criterion(logits_multiclass, targets)\n",
    "            \n",
    "            # Weighted combination of the losses\n",
    "            loss = (loss_binary * lw[0] + loss_multiclass * lw[1]) / sum(lw)\n",
    "            \n",
    "            # If mixup is performed, adjust the loss\n",
    "            if do_mixup:\n",
    "                loss_binary_mix = binary_criterion(logits_binary, targets_mix)\n",
    "                loss_multiclass_mix = multiclass_criterion(logits_multiclass, targets_mix)\n",
    "                \n",
    "                loss = loss * lam + (loss_binary_mix * lw[0] + loss_multiclass_mix * lw[1]) / sum(lw) * (1 - lam)\n",
    "\n",
    "        # Store the losses for logging\n",
    "        binary_loss_list.append(loss_binary.item())\n",
    "        multiclass_loss_list.append(loss_multiclass.item())\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler_cosine.step()\n",
    "\n",
    "        bar.set_description(f'Binary Loss: {np.mean(binary_loss_list[-30:]):.4f}, Multiclass Loss: {np.mean(multiclass_loss_list[-30:]):.4f}')\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "def valid_func(model, loader_valid):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    valid_loss = []\n",
    "    binary_loss_list = []\n",
    "    multiclass_loss_list = []\n",
    "    bar = tqdm(loader_valid)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for images, targets in bar:\n",
    "            images = images.to(model_device)\n",
    "            targets = targets.to(model_device)\n",
    "\n",
    "            logits_binary, logits_multiclass = model(images)\n",
    "            \n",
    "            # Compute individual losses\n",
    "            loss_binary = binary_criterion(logits_binary, targets)\n",
    "            loss_multiclass = multiclass_criterion(logits_multiclass, targets)\n",
    "            \n",
    "            # Weighted combination of the losses\n",
    "            loss = (loss_binary * lw[0] + loss_multiclass * lw[1]) / sum(lw)\n",
    "\n",
    "            # Store the losses for logging\n",
    "            binary_loss_list.append(loss_binary.item())\n",
    "            multiclass_loss_list.append(loss_multiclass.item())\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "            bar.set_description(f'Binary Loss: {np.mean(binary_loss_list):.4f}, Multiclass Loss: {np.mean(multiclass_loss_list):.4f}')\n",
    "\n",
    "    return np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0294bd1-0ee2-4447-852a-dbfcc937806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "# Initialize the stratifier\n",
    "stratifier = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2908b118-eb05-4810-bf1f-bd3ab7bc4acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mixup(input, truth, clip=[0, 1]):\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    \n",
    "    # Assuming truth is in the shape (batch_size, n_labels)\n",
    "    # where n_labels includes binary and multiclass labels\n",
    "    shuffled_labels = truth[indices]\n",
    "    \n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    mixed_input = input * lam + shuffled_input * (1 - lam)\n",
    "    \n",
    "    # Mix labels\n",
    "    mixed_labels = truth * lam + shuffled_labels * (1 - lam)\n",
    "    \n",
    "    return mixed_input, mixed_labels, shuffled_labels, lam\n",
    "\n",
    "def binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    bowel_logits = logits[:, 0]\n",
    "    extravasation_logits = logits[:, 1]\n",
    "\n",
    "    # Split targets\n",
    "    bowel_targets = targets[:, 0]\n",
    "    extravasation_targets = targets[:, 1]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation\n",
    "    bowel_losses = nn.BCEWithLogitsLoss(reduction='none')(bowel_logits, bowel_targets)\n",
    "    bowel_losses[bowel_targets > 0] *= 2.\n",
    "    \n",
    "    extravasation_losses = nn.BCEWithLogitsLoss(reduction='none')(extravasation_logits, extravasation_targets)\n",
    "    extravasation_losses[extravasation_targets > 0] *= 2.\n",
    "\n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = torch.cat([bowel_losses, extravasation_losses], dim=0)\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[torch.cat([bowel_targets, extravasation_targets], dim=0) > 0] *= 2\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def extravasation_binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    extravasation_logits = logits[:, 1]\n",
    "\n",
    "    # Split targets\n",
    "    extravasation_targets = targets[:, 1]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation  \n",
    "    extravasation_losses = nn.BCEWithLogitsLoss(reduction='none')(extravasation_logits, extravasation_targets)\n",
    "    extravasation_losses[extravasation_targets > 0] *= 2.\n",
    "\n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = extravasation_losses\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[extravasation_losses > 0] *= 2\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "    \n",
    "def bowel_binary_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    bowel_logits = logits[:, 0]\n",
    "\n",
    "    # Split targets\n",
    "    bowel_targets = targets[:, 0]\n",
    "    \n",
    "    # Binary Loss for bowel and extravasation\n",
    "    bowel_losses = nn.BCEWithLogitsLoss(reduction='none')(bowel_logits, bowel_targets)\n",
    "    bowel_losses[bowel_targets > 0] *= 2.\n",
    "    \n",
    "    # Combine and normalize the binary losses\n",
    "    all_losses = bowel_losses\n",
    "    norm = torch.ones(all_losses.shape[0]).to(model_device)\n",
    "    norm[bowel_targets > 0] *= 2\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def compute_organ_criterion(logits, targets, slice_indices, model_device):\n",
    "    organ_logits = logits[:, slice_indices]\n",
    "    organ_targets = targets[:, slice_indices]\n",
    "\n",
    "    # Multiclass Loss\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    organ_losses = ce(organ_logits, organ_targets)\n",
    "\n",
    "    # Normalization\n",
    "    norm = torch.ones(organ_losses.shape[0]).to(model_device)\n",
    "    organ_targets_flat = organ_targets.argmax(dim=1)\n",
    "    norm[organ_targets_flat > 0] *= 2\n",
    "    \n",
    "    # Calculate the final loss\n",
    "    total_loss = organ_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def kidney_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(0,3), model_device)\n",
    "\n",
    "def liver_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(3,6), model_device)\n",
    "\n",
    "def spleen_multi_criterion(logits, targets, model_device):\n",
    "    return compute_organ_criterion(logits, targets, slice(6,9), model_device)\n",
    "    \n",
    "def multiclass_criterion(logits, targets, model_device):\n",
    "    # Split logits\n",
    "    print(logits.shape)\n",
    "    \n",
    "    kidney_logits = logits[:, 0:3]  # Classes 0, 1, 2\n",
    "    liver_logits = logits[:, 3:6]   # Classes 3, 4, 5\n",
    "    spleen_logits = logits[:, 6:9]  # Classes 6, 7, 8\n",
    "\n",
    "    # Split targets\n",
    "    kidney_targets = targets[:, 0:3]\n",
    "    liver_targets = targets[:, 3:6]\n",
    "    spleen_targets = targets[:, 6:9]\n",
    "\n",
    "    # Multiclass Losses\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "    kidney_losses = ce(kidney_logits, kidney_targets)\n",
    "    liver_losses = ce(liver_logits, liver_targets)\n",
    "    spleen_losses = ce(spleen_logits, spleen_targets)\n",
    "\n",
    "    # Combine and normalize the multiclass losses\n",
    "    all_losses = torch.cat([kidney_losses, liver_losses, spleen_losses], dim=0)\n",
    "    norm = torch.ones(all_losses.shape[0]*3).to(model_device)\n",
    "\n",
    "    # Reshape each organ's targets\n",
    "    kidney_targets_flat = kidney_targets.reshape(-1)\n",
    "    liver_targets_flat = liver_targets.reshape(-1)\n",
    "    spleen_targets_flat = spleen_targets.reshape(-1)\n",
    "    \n",
    "    # Concatenate the reshaped targets\n",
    "    combined_targets = torch.cat([kidney_targets_flat, liver_targets_flat, spleen_targets_flat])\n",
    "    \n",
    "    # Update the norm based on the targets\n",
    "    norm[combined_targets > 0] *= 2\n",
    "    \n",
    "    # Calculate the final loss\n",
    "    total_loss = all_losses.sum() / norm.sum()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd0003e1-427f-4131-acc6-5e0f8216786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patients, test_patients = next(stratifier.split(y_original_format, y_original_format))\n",
    "\n",
    "train_df = merged_df[merged_df['patient_id'].isin(train_patients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "052b9f60-77b5-4a76-b3f3-9446cbab6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_cases = train_df[(train_df['extravasation_injury']==0) \n",
    "            & (train_df['kidney_healthy']==1)\n",
    "            & (train_df['liver_healthy']==1)\n",
    "            & (train_df['spleen_healthy']==1)\n",
    "            & (train_df['bowel_injury']==0)].series_id.unique()\n",
    "\n",
    "ids_to_remove = random.sample(list(normal_cases), len(train_df.series_id.unique())%4)\n",
    "\n",
    "train_df = train_df[~train_df['series_id'].isin(ids_to_remove)]\n",
    "train_dataset = OrganTrainDataset(train_df, transform=transforms_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d650550-5367-4dd8-894a-ef2f9c6d61b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/40 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 60, 3, 224, 224])\n",
      "torch.Size([4, 11])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bar = tqdm(train_dataloader)\n",
    "targets = None\n",
    "images = None\n",
    "for images, targets in bar:\n",
    "    print(images.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b895b92-a92e-4f84-8d65-07df0829ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n",
      "Thu Oct 12 08:25:52 2023 Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/39 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Train Loss: 0.5536: 100%|██████████████████████████████████████████████████████████████████████████████████| 39/39 [01:37<00:00,  2.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = TimmModelWithDualLSTM(backbone=backbone, pretrained=True)\n",
    "model.to(model_device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "init_lr = 23e-5\n",
    "eta_min = 23e-6\n",
    "n_epochs = 10\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs, eta_min=eta_min)\n",
    "\n",
    "use_amp = True\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "log_dir = 'logs'\n",
    "model_dir = 'best_lstm_model'\n",
    "n_epochs = 10\n",
    "DEBUG = True\n",
    "p_mixup = 0.5\n",
    "fold = 0\n",
    "kernel_type ='not_real'\n",
    "lw = [2,1]\n",
    "\n",
    "log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "\n",
    "train_patients, test_patients = next(stratifier.split(y_original_format, y_original_format))\n",
    "\n",
    "train_df = merged_df[merged_df['patient_id'].isin(train_patients)]\n",
    "\n",
    "normal_cases = train_df[(train_df['extravasation_injury']==0) \n",
    "            & (train_df['kidney_healthy']==1)\n",
    "            & (train_df['liver_healthy']==1)\n",
    "            & (train_df['spleen_healthy']==1)\n",
    "            & (train_df['bowel_injury']==0)].series_id.unique()\n",
    "\n",
    "ids_to_remove = random.sample(list(normal_cases), len(train_df.series_id.unique())%4)\n",
    "\n",
    "train_df = train_df[~train_df['series_id'].isin(ids_to_remove)]\n",
    "train_dataset = OrganTrainDataset(train_df, transform=transforms_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# test_df = merged_df[merged_df['patient_id'].isin(test_patients)]\n",
    "# test_dataset = OrganTrainDataset(test_df, transform=transforms_valid)\n",
    "# test_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "metric_best = np.inf\n",
    "loss_min = np.inf\n",
    "\n",
    "# for epoch in range(1, n_epochs+1):\n",
    "epoch = 1\n",
    "\n",
    "print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "model.train()\n",
    "train_loss = []\n",
    "binary_loss_list = []\n",
    "multiclass_loss_list = []\n",
    "\n",
    "bowel_binary_loss_list = []\n",
    "extravasation_binary_loss_list = []\n",
    "kidney_multi_loss_list = []\n",
    "liver_multi_loss_list = []\n",
    "spleen_multi_loss_list = []\n",
    "\n",
    "bar = tqdm(train_dataloader)\n",
    "\n",
    "for images, targets in bar:\n",
    "    images = images.to(model_device)\n",
    "    targets = targets.to(model_device)\n",
    "\n",
    "    do_mixup = False\n",
    "    if random.random() < p_mixup:\n",
    "        do_mixup = True\n",
    "        images, targets, targets_mix, lam = mixup(images, targets)\n",
    "\n",
    "    # print(torch.unique(targets))\n",
    "\n",
    "    with amp.autocast():\n",
    "        logits_binary, logits_multiclass = model(images)\n",
    "        \n",
    "        # Compute individual losses\n",
    "        bowel_binary_loss = bowel_binary_criterion(logits_binary, targets, model_device)\n",
    "        \n",
    "        extravasation_binary_loss = extravasation_binary_criterion(logits_binary, targets, model_device)\n",
    "        \n",
    "        kidney_multi_loss = kidney_multi_criterion(logits_multiclass, targets, model_device)\n",
    "\n",
    "        liver_multi_loss = liver_multi_criterion(logits_multiclass, targets, model_device)\n",
    "\n",
    "        spleen_multi_loss = spleen_multi_criterion(logits_multiclass, targets, model_device)\n",
    "    \n",
    "        # Define your weights\n",
    "        w_bowel = 1\n",
    "        w_extravasation = 0.5\n",
    "        w_kidney = 3.0\n",
    "        w_liver = 3.0\n",
    "        w_spleen = 3.0\n",
    "        \n",
    "        # Compute the weighted combination\n",
    "        loss = (w_bowel * bowel_binary_loss +\n",
    "                         w_extravasation * extravasation_binary_loss +\n",
    "                         w_kidney * kidney_multi_loss +\n",
    "                         w_liver * liver_multi_loss +\n",
    "                         w_spleen * spleen_multi_loss) / (w_bowel + w_extravasation + w_kidney + w_liver + w_spleen)\n",
    "        #print(loss)\n",
    "        \n",
    "        # If mixup is performed, adjust the loss\n",
    "        if do_mixup:\n",
    "            bowel_binary_mix = bowel_binary_criterion(logits_binary, targets_mix, model_device)\n",
    "            \n",
    "            extravasation_binary_mix = extravasation_binary_criterion(logits_binary, targets_mix, model_device)\n",
    "            \n",
    "            kidney_multi_mix = kidney_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "    \n",
    "            liver_multi_mix = liver_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "    \n",
    "            spleen_multi_mix = spleen_multi_criterion(logits_multiclass, targets_mix, model_device)\n",
    "            \n",
    "            loss = loss * lam + (w_bowel * bowel_binary_loss +\n",
    "                                     w_extravasation * extravasation_binary_loss +\n",
    "                                     w_kidney * kidney_multi_loss +\n",
    "                                     w_liver * liver_multi_loss +\n",
    "                                     w_spleen * spleen_multi_loss) / (w_bowel + w_extravasation + w_kidney + w_liver + w_spleen) * (1 - lam)\n",
    "\n",
    "    # Store the losses for logging\n",
    "    # binary_loss_list.append(loss_binary.item())\n",
    "    # multiclass_loss_list.append(loss_multiclass.item())\n",
    "    bowel_binary_loss_list.append(bowel_binary_loss.item())\n",
    "    \n",
    "    extravasation_binary_loss_list.append(extravasation_binary_loss.item())\n",
    "    \n",
    "    kidney_multi_loss_list.append(kidney_multi_loss.item()) \n",
    "\n",
    "    liver_multi_loss_list.append(liver_multi_loss.item()) \n",
    "\n",
    "    spleen_multi_loss_list.append(spleen_multi_loss.item())\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    scheduler_cosine.step()\n",
    "\n",
    "    # if train_loss and bowel_binary_loss and extravasation_binary_loss and kidney_multi_loss and liver_multi_loss and spleen_multi_loss:\n",
    "    bar.set_description(f'Train Loss: {np.mean(train_loss):.4f}')#, Bowel Loss: {np.mean(bowel_binary_loss):.4f}, Ext Loss: {np.mean(extravasation_binary_loss):.4f}, Kidney Loss: {np.mean(kidney_multi_loss):.4f}, Liver Loss: {np.mean(liver_multi_loss):.4f}, Spleen Loss: {np.mean(spleen_multi_loss):.4f}')\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634e94de-4d54-4ef3-9293-97f5e4158164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9509a481-73b5-4810-962a-2eb9f69fd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = TimmModelWithDualLSTM(backbone=backbone)\n",
    "\n",
    "# Create dummy tensor\n",
    "bs = 2  # example batch size\n",
    "n_slice_per_c = 15\n",
    "n_organs = 4\n",
    "in_chans = 3\n",
    "image_size = 224\n",
    "dummy_tensor = torch.randn(bs, n_slice_per_c * n_organs * in_chans, image_size, image_size)\n",
    "# dummy_tensor.half()\n",
    "# Pass dummy tensor through the model\n",
    "output1, output2 = model(dummy_tensor)\n",
    "#print(output1.shape, output2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615d94cf-2aca-4411-ad93-9339c0272f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) torch.Size([2, 9])\n"
     ]
    }
   ],
   "source": [
    "print(output1.shape, output2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
