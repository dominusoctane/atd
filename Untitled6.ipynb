{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1234bc8-7cae-4878-8a70-866480f685cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    ThreadDataLoader,\n",
    "    Dataset,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.layers import Act, Norm\n",
    "from monai.networks.nets import UNet, SwinUNETR, AHNet, VNet\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    EnsureTyped,\n",
    "    FgBgToIndicesd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"  # Use the 3rd and 4th GPU. Indexing starts from 0.\n",
    "\n",
    "# for profiling\n",
    "import nvtx\n",
    "from monai.utils.nvtx import Range\n",
    "import contextlib  # to improve code readability (combining training/validation loop with and without profiling)\n",
    "\n",
    "#print_config()\n",
    "set_determinism(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d363ad3-56ed-4b36-a343-39ff571bd80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir is: /tmp/tmpsda545t9\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(f\"root dir is: {root_dir}\")\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "out_dir = os.path.join(current_directory,\"outputs/\")\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375aee7-3d7f-4d88-a9ad-92002569b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import time\n",
    "import timm\n",
    "import pickle\n",
    "import random\n",
    "import pydicom\n",
    "import argparse\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import albumentations\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from timm.layers.conv2d_same import Conv2dSame\n",
    "\n",
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "\n",
    "# Calculate symmetric padding for a convolution\n",
    "def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n",
    "    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n",
    "    return padding\n",
    "\n",
    "\n",
    "# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n",
    "def get_same_padding(x: int, k: int, s: int, d: int):\n",
    "    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n",
    "\n",
    "\n",
    "# Can SAME padding for given args be done statically?\n",
    "def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n",
    "    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n",
    "\n",
    "\n",
    "# Dynamically pad input x with 'SAME' padding for conv with specified args\n",
    "def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1, 1), value: float = 0):\n",
    "    ih, iw, iz = x.size()[-3:]\n",
    "    pad_h = get_same_padding(ih, k[0], s[0], d[0])\n",
    "    pad_w = get_same_padding(iw, k[1], s[1], d[1])\n",
    "    pad_z = get_same_padding(iz, k[2], s[2], d[2])\n",
    "    if pad_h > 0 or pad_w > 0 or pad_z > 0:\n",
    "        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2, pad_z // 2, pad_z - pad_z // 2], value=value)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n",
    "    dynamic = False\n",
    "    if isinstance(padding, str):\n",
    "        # for any string padding, the padding will be calculated for you, one of three ways\n",
    "        padding = padding.lower()\n",
    "        if padding == 'same':\n",
    "            # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact\n",
    "            if is_static_pad(kernel_size, **kwargs):\n",
    "                # static case, no extra overhead\n",
    "                padding = get_padding(kernel_size, **kwargs)\n",
    "            else:\n",
    "                # dynamic 'SAME' padding, has runtime/GPU memory overhead\n",
    "                padding = 0\n",
    "                dynamic = True\n",
    "        elif padding == 'valid':\n",
    "            # 'VALID' padding, same as padding=0\n",
    "            padding = 0\n",
    "        else:\n",
    "            # Default to PyTorch style 'same'-ish symmetric padding\n",
    "            padding = get_padding(kernel_size, **kwargs)\n",
    "    return padding, dynamic\n",
    "\n",
    "\n",
    "def conv3d_same(\n",
    "        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int, int] = (1, 1, 1),\n",
    "        padding: Tuple[int, int, int] = (0, 0, 0), dilation: Tuple[int, int, int] = (1, 1, 1), groups: int = 1):\n",
    "    x = pad_same(x, weight.shape[-3:], stride, dilation)\n",
    "    return F.conv3d(x, weight, bias, stride, (0, 0, 0), dilation, groups)\n",
    "\n",
    "\n",
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3d convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "def create_conv3d_pad(in_chs, out_chs, kernel_size, **kwargs):\n",
    "    padding = kwargs.pop('padding', '')\n",
    "    kwargs.setdefault('bias', False)\n",
    "    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n",
    "    if is_dynamic:\n",
    "        return Conv3dSame(in_chs, out_chs, kernel_size, **kwargs)\n",
    "    else:\n",
    "        return nn.Conv3d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)\n",
    "\n",
    "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "n_folds = 5\n",
    "backbone = 'resnet18d'\n",
    "\n",
    "init_lr = 3e-3\n",
    "batch_size = 4\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "out_dim = 6\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False, num_classes=6):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.1,\n",
    "            drop_path_rate=0.1,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        # Modify the first layer of the encoder for 1-channel input\n",
    "        self.encoder.conv1 = nn.Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
    "\n",
    "        self.decoder = smp.Unet(\n",
    "            encoder_name=backbone,\n",
    "            encoder_depth=5,\n",
    "            encoder_weights=None,  # Weights are already loaded in the encoder part\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=[256, 128, 64, 32, 16],\n",
    "            decoder_attention_type='scse',\n",
    "            in_channels=1,\n",
    "            classes=num_classes,\n",
    "            activation=None,  # Use 'None' because we handle activation in the loss function (e.g., CrossEntropy)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657dbbf-3914-4086-8778-d385c0539037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f469e8f-ad7b-4b82-adc5-e0cef59a6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_get_bounding_boxes_v6(tensor):\n",
    "    \"\"\"\n",
    "    Compute bounding boxes, determine the voxel count per depth, and the inner region for each organ.\n",
    "    Also computes a bounding box that encompasses all organs based on the inner_slices.\n",
    "    \n",
    "    Args:\n",
    "    - tensor (numpy.ndarray): The input tensor with shape (D, H, W).\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Bounding boxes, voxel counts per depth, and inner regions for each organ.\n",
    "            Also includes the encompassing bounding box for all organs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize bounding boxes dictionary\n",
    "    bounding_boxes = {}\n",
    "    \n",
    "    # List to store all coordinates within the inner_slices for all organs\n",
    "    all_coords_in_inner_slices = []\n",
    "    \n",
    "    for organ_label, organ_name in SEGMENTATION_CODES.items():\n",
    "        if organ_label == 0:  # Skip background\n",
    "            continue\n",
    "        \n",
    "        # Find the slices where the organ appears\n",
    "        slices = np.where(np.any(tensor == organ_label, axis=(1, 2)))[0]\n",
    "        \n",
    "        if len(slices) == 0:  # If organ does not appear, continue to next organ\n",
    "            continue\n",
    "        \n",
    "        # Compute voxel count for each slice and store in a dictionary with slice index as key\n",
    "        voxel_counts_per_depth = {slice_idx: np.sum(tensor[slice_idx] == organ_label) for slice_idx in slices}\n",
    "        \n",
    "        # Fourier-based smoothing and thresholding\n",
    "        simple_list = list(voxel_counts_per_depth.values())\n",
    "        smoothed_list = fourier_smoothing(simple_list, fourier_factor)\n",
    "        \n",
    "        # Thresholding logic\n",
    "        max_value = np.max(smoothed_list)\n",
    "        threshold = threshold_factor * max_value\n",
    "        in_threshold = (smoothed_list >= threshold)\n",
    "        \n",
    "        # Extract slice indices that are in the inner region\n",
    "        inner_slices = get_contiguous_inner_slices(in_threshold, slices)\n",
    "        \n",
    "        # Gather all coordinates for this organ that are within the inner_slices\n",
    "        coords = np.argwhere((tensor == organ_label) & (np.isin(tensor, inner_slices)))\n",
    "        all_coords_in_inner_slices.extend(coords)\n",
    "        \n",
    "        # Bounding box computation for this organ\n",
    "        min_coords = coords.min(axis=0)\n",
    "        max_coords = coords.max(axis=0)\n",
    "        \n",
    "        bounding_boxes[organ_name] = {\n",
    "            \"top_left_front\": tuple(min_coords),\n",
    "            \"bottom_right_back\": tuple(max_coords),\n",
    "            \"depth_range\": (min_coords[0], max_coords[0]),\n",
    "            \"voxel_counts_per_depth\": voxel_counts_per_depth,\n",
    "            \"inner_slices\": inner_slices\n",
    "        }\n",
    "    \n",
    "    # Compute the encompassing bounding box for all organs based on inner_slices\n",
    "    all_coords_array = np.array(all_coords_in_inner_slices)\n",
    "    min_encompassing_coords = all_coords_array.min(axis=0)\n",
    "    max_encompassing_coords = all_coords_array.max(axis=0)\n",
    "    bounding_boxes[\"encompassing\"] = {\n",
    "        \"top_left_front\": tuple(min_encompassing_coords),\n",
    "        \"bottom_right_back\": tuple(max_encompassing_coords),\n",
    "        \"depth_range\": (min_encompassing_coords[0], max_encompassing_coords[0])\n",
    "    }\n",
    "        \n",
    "    return bounding_boxes\n",
    "\n",
    "def fourier_smoothing(y, cutoff_fraction=0.1):\n",
    "    # Compute the FFT of the input data\n",
    "    y_fft = np.fft.fft(y)\n",
    "    \n",
    "    # Zero out the higher frequencies\n",
    "    cutoff_idx = int(cutoff_fraction * len(y_fft))\n",
    "    y_fft[cutoff_idx:-cutoff_idx] = 0\n",
    "    \n",
    "    # Compute the inverse FFT to get the smoothed data\n",
    "    y_smoothed = np.fft.ifft(y_fft).real  # Only take the real part\n",
    "    \n",
    "    return y_smoothed\n",
    "    \n",
    "def get_contiguous_inner_slices(in_threshold, slices):\n",
    "    \"\"\"\n",
    "    Get contiguous blocks of slices that are above the threshold.\n",
    "    \n",
    "    Args:\n",
    "    - in_threshold (list): A boolean list indicating if the slice is above the threshold.\n",
    "    - slices (list): List of slice indices.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Contiguous inner slices.\n",
    "    \"\"\"\n",
    "    inner_slices = []\n",
    "    start_slice = None\n",
    "\n",
    "    for i, val in enumerate(in_threshold):\n",
    "        if val:\n",
    "            if start_slice is None:  # Start of a new block\n",
    "                start_slice = slices[i]\n",
    "        else:\n",
    "            if start_slice is not None:  # End of the current block\n",
    "                inner_slices.extend(range(start_slice, slices[i]))\n",
    "                start_slice = None\n",
    "\n",
    "    if start_slice is not None:  # For the case where the last block reaches the end\n",
    "        inner_slices.extend(range(start_slice, slices[-1] + 1))\n",
    "\n",
    "    return inner_slices\n",
    "\n",
    "# Function to save numpy array as .nii.gz\n",
    "def save_nii(data, filename):\n",
    "    img = nib.Nifti1Image(data, affine=np.eye(4))\n",
    "    nib.save(img, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c622314-4dd0-4157-992a-2f16588376c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_device = torch.device(\"cuda:1\") \n",
    "\n",
    "# Create the model\n",
    "seg_model = TimmSegModel(backbone = 'resnet18', segtype='unet', pretrained = True)\n",
    "seg_model = convert_3d(seg_model)\n",
    "# Test the model (just to ensure everything is working)\n",
    "seg_model = seg_model.to(model_device)\n",
    "\n",
    "seg_model.load_state_dict(torch.load(os.path.join(current_directory, \"resnet18_unet_best_metric_model.pt\")))\n",
    "seg_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053e2f8-4749-42a0-abbf-f8fd44104b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation pipeline\n",
    "test_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=\"image\"),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(2.0, 2.0, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688db7f8-6495-47aa-b768-5ee24d27678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def squash(inputs, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param inputs: vectors to be squashed\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same size as inputs\n",
    "    \"\"\"\n",
    "    norm = torch.norm(inputs, p=2, dim=axis, keepdim=True)\n",
    "    scale = norm**2 / (1 + norm**2) / (norm + 1e-8)\n",
    "    return scale * inputs\n",
    "\n",
    "\n",
    "class DenseCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    The dense capsule layer. It is similar to Dense (FC) layer. Dense layer has `in_num` inputs, each is a scalar, the\n",
    "    output of the neuron from the former layer, and it has `out_num` output neurons. DenseCapsule just expands the\n",
    "    output of the neuron from scalar to vector. So its input size = [None, in_num_caps, in_dim_caps] and output size = \\\n",
    "    [None, out_num_caps, out_dim_caps]. For Dense Layer, in_dim_caps = out_dim_caps = 1.\n",
    "\n",
    "    :param in_num_caps: number of cpasules inputted to this layer\n",
    "    :param in_dim_caps: dimension of input capsules\n",
    "    :param out_num_caps: number of capsules outputted from this layer\n",
    "    :param out_dim_caps: dimension of output capsules\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, routings=3):\n",
    "        super(DenseCapsule, self).__init__()\n",
    "        self.in_num_caps = in_num_caps\n",
    "        self.in_dim_caps = in_dim_caps\n",
    "        self.out_num_caps = out_num_caps\n",
    "        self.out_dim_caps = out_dim_caps\n",
    "        self.routings = routings\n",
    "        self.weight = nn.Parameter(0.01 * torch.randn(out_num_caps, in_num_caps, out_dim_caps, in_dim_caps))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.size=[batch, in_num_caps, in_dim_caps]\n",
    "        # expanded to    [batch, 1,            in_num_caps, in_dim_caps,  1]\n",
    "        # weight.size   =[       out_num_caps, in_num_caps, out_dim_caps, in_dim_caps]\n",
    "        # torch.matmul: [out_dim_caps, in_dim_caps] x [in_dim_caps, 1] -> [out_dim_caps, 1]\n",
    "        # => x_hat.size =[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "        x_hat = torch.squeeze(torch.matmul(self.weight, x[:, None, :, :, None]), dim=-1)\n",
    "\n",
    "        # In forward pass, `x_hat_detached` = `x_hat`;\n",
    "        # In backward, no gradient can flow from `x_hat_detached` back to `x_hat`.\n",
    "        x_hat_detached = x_hat.detach()\n",
    "\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        # b.size = [batch, out_num_caps, in_num_caps]\n",
    "        b = Variable(torch.zeros(x.size(0), self.out_num_caps, self.in_num_caps)).cuda()\n",
    "\n",
    "        assert self.routings > 0, 'The \\'routings\\' should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # c.size = [batch, out_num_caps, in_num_caps]\n",
    "            c = F.softmax(b, dim=1)\n",
    "\n",
    "            # At last iteration, use `x_hat` to compute `outputs` in order to backpropagate gradient\n",
    "            if i == self.routings - 1:\n",
    "                # c.size expanded to [batch, out_num_caps, in_num_caps, 1           ]\n",
    "                # x_hat.size     =   [batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => outputs.size=   [batch, out_num_caps, 1,           out_dim_caps]\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat))  # alternative way\n",
    "            else:  # Otherwise, use `x_hat_detached` to update `b`. No gradients flow on this path.\n",
    "                outputs = squash(torch.sum(c[:, :, :, None] * x_hat_detached, dim=-2, keepdim=True))\n",
    "                # outputs = squash(torch.matmul(c[:, :, None, :], x_hat_detached))  # alternative way\n",
    "\n",
    "                # outputs.size       =[batch, out_num_caps, 1,           out_dim_caps]\n",
    "                # x_hat_detached.size=[batch, out_num_caps, in_num_caps, out_dim_caps]\n",
    "                # => b.size          =[batch, out_num_caps, in_num_caps]\n",
    "                b = b + torch.sum(outputs * x_hat_detached, dim=-1)\n",
    "\n",
    "        return torch.squeeze(outputs, dim=-2)\n",
    "\n",
    "\n",
    "class PrimaryCapsule(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply Conv2D with `out_channels` and then reshape to get capsules\n",
    "    :param in_channels: input channels\n",
    "    :param out_channels: output channels\n",
    "    :param dim_caps: dimension of capsule\n",
    "    :param kernel_size: kernel size\n",
    "    :return: output tensor, size=[batch, num_caps, dim_caps]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0):\n",
    "        super(PrimaryCapsule, self).__init__()\n",
    "        self.dim_caps = dim_caps\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.conv2d(x)\n",
    "        outputs = outputs.view(x.size(0), -1, self.dim_caps)\n",
    "        return squash(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae66196-26cf-4d4a-89c1-ab29e3694509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class CombinedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, pretrained=True, unet_feature_dim=1024, capsnet_feature_dim=1024):\n",
    "        super(CombinedFeatureExtractor, self).__init__()\n",
    "\n",
    "        # Use pre-trained ResNet as encoder but modify for desired input channels\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-1])  # Remove the classification head\n",
    "\n",
    "        # Decoder (U-Net style)\n",
    "        self.decoder_block1 = self.conv_block(512, 256)\n",
    "        self.decoder_block2 = self.conv_block(256, 128)\n",
    "        self.decoder_block3 = self.conv_block(128, 64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, input_channels, 256, 256)\n",
    "            x = self.encoder(x)\n",
    "            x = self.decoder_block1(F.interpolate(x, scale_factor=2))\n",
    "            x = self.decoder_block2(F.interpolate(x, scale_factor=2))\n",
    "            x = self.decoder_block3(F.interpolate(x, scale_factor=2))\n",
    "        \n",
    "        # Calculate the number of features and initialize the fully connected layer\n",
    "        num_features = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        \n",
    "        # Calculate the number of features for U-Net and initialize the fully connected layer\n",
    "        self.fc_unet = nn.Linear(num_features, unet_feature_dim)  # Assuming 256x256 input size\n",
    "        \n",
    "        # CapsNet Layer\n",
    "        self.primary_capsules = PrimaryCapsule(in_channels=512, out_channels=256, dim_caps=8, kernel_size=1)\n",
    "        \n",
    "        self.digit_capsules = DenseCapsule(in_num_caps=2048, \n",
    "                                           in_dim_caps=self.primary_capsules.dim_caps,\n",
    "                                           out_num_caps=capsnet_feature_dim // 8,  # Adjust the number of capsules based on desired feature dimension\n",
    "                                           out_dim_caps=8, \n",
    "                                           routings=3)\n",
    "        \n",
    "        self.fc_caps = nn.Linear(capsnet_feature_dim, capsnet_feature_dim)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (pre-trained CNN)\n",
    "        encoded_x = self.encoder(x)\n",
    "        # Decoder (U-Net)\n",
    "        unet_features = self.decoder_block1(F.interpolate(encoded_x, scale_factor=2))\n",
    "        unet_features = self.decoder_block2(F.interpolate(unet_features, scale_factor=2))\n",
    "        unet_features = self.decoder_block3(F.interpolate(unet_features, scale_factor=2))\n",
    "        \n",
    "        # Flatten and pass through the fully connected layer for 1D feature vector\n",
    "        unet_features = unet_features.view(unet_features.size(0), -1)\n",
    "        unet_features = self.fc_unet(unet_features)\n",
    " \n",
    "        x_reshaped = encoded_x.view(encoded_x.size(0), 512, 1, 1)\n",
    "        x_reshaped = F.interpolate(x_reshaped, size=(8, 8), mode='nearest')  # Upsample to [batch_size, 512, 7, 7]\n",
    "\n",
    "        print(x_reshaped.shape)\n",
    "        caps_features = self.primary_capsules(x_reshaped)\n",
    "        print(caps_features.shape)\n",
    "\n",
    "        caps_features = self.digit_capsules(caps_features)\n",
    "        caps_features = self.fc_caps(caps_features.view(caps_features.size(0), -1))\n",
    "\n",
    "        # Combine the features from U-Net and CapsNet\n",
    "        combined_features = torch.cat([unet_features, caps_features], dim=1)\n",
    "\n",
    "        return unet_features, caps_features\n",
    "\n",
    "def extract_encompassing_subregion(volume, bounding_boxes):\n",
    "    # Extract the coordinates from the 'encompassing' bounding box\n",
    "    top_left_front = bounding_boxes[\"encompassing\"][\"top_left_front\"]\n",
    "    bottom_right_back = bounding_boxes[\"encompassing\"][\"bottom_right_back\"]\n",
    "\n",
    "    # Use the coordinates to slice into the volume tensor\n",
    "    subregion = volume[top_left_front[0]:bottom_right_back[0]+1, \n",
    "                       top_left_front[1]:bottom_right_back[1]+1, \n",
    "                       top_left_front[2]:bottom_right_back[2]+1]\n",
    "\n",
    "    return subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957745bd-e024-4626-9bfe-0b4891841f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "train_info = pd.read_csv('train.csv')\n",
    "train_info.patient_id = train_info.patient_id.astype(str)\n",
    "\n",
    "curr_df = dicom_df[dicom_df.patient_duplicate_count == 2]\n",
    "train_info = train_info[train_info.patient_id.isin(curr_df.patient_id)]\n",
    "train_info = train_info[train_info.extravasation_injury == 1]\n",
    "curr_df = curr_df[curr_df.patient_id.isin(train_info.patient_id)]\n",
    "\n",
    "unique_patients = np.unique(curr_df.patient_id)\n",
    "\n",
    "save_directory = \"/workspace/0728tot/ATD/patient_nii\"  # Adjust to your path\n",
    "\n",
    "# Directory where the .nii.gz files are saved\n",
    "vol_directory = \"/workspace/0728tot/ATD/patient_nii\"\n",
    "\n",
    "# Iterate over unique patients\n",
    "for patient_id in tqdm(unique_patients[0:1]):\n",
    "    eval_df = curr_df[curr_df.patient_id == patient_id]\n",
    "    paths = np.array(eval_df.series_dir)\n",
    "    series_names = np.array(eval_df.series_id)\n",
    "    \n",
    "    volumes = []\n",
    "    \n",
    "    # For each series in a patient, sample the volume\n",
    "    for path, series_name in zip(paths, series_names):\n",
    "        vol = sample_patient_volume(path, depth_downsample_rate=None, lw_downsample_rate=None, \n",
    "                                  adjust_pixel_spacing=\"no\", standardize_pixel_array=\"yes\", \n",
    "                                  target_pixel_spacing=[1.0, 1.0], target_thickness=1)\n",
    "        volumes.append((series_name, vol))\n",
    "    \n",
    "    # Filtering based on depth criterion\n",
    "    depths = [vol.shape[0] for _, vol in volumes]\n",
    "    max_depth = max(depths)\n",
    "    \n",
    "    # Remove volumes that don't meet the depth criterion from the list\n",
    "    volumes = [(series_name, vol) for series_name, vol in volumes if vol.shape[0] >= 0.4 * max_depth]\n",
    "\n",
    "    # Save volumes and keep track of saved filepaths\n",
    "    saved_filepaths = []\n",
    "    \n",
    "    for series_name, vol in volumes:\n",
    "        filename = os.path.join(vol_directory, f\"{series_name}.nii.gz\")\n",
    "        save_nii(vol, filename)\n",
    "        saved_filepaths.append(filename)\n",
    "    \n",
    "    # Create a data list for the Dataset using the saved_filepaths list\n",
    "    data_list = [{\"image\": filepath} for filepath in saved_filepaths]\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_org_ds = Dataset(data=data_list, transform=test_org_transforms)\n",
    "    test_org_loader = DataLoader(test_org_ds, batch_size=1, num_workers=0)\n",
    "    \n",
    "    # Initialize an empty list to store bounding box data\n",
    "    bbox_data_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Loop through both filepaths (from DataLoader) and volumes\n",
    "        for (test_data, (series_name, vol)) in zip(test_org_loader, volumes):\n",
    "\n",
    "            test_inputs = test_data[\"image\"].to(model_device)\n",
    "            roi_size = (96, 96, 96)\n",
    "            sw_batch_size = 4\n",
    "            curr_data = sliding_window_inference(test_inputs, roi_size, sw_batch_size, seg_model)\n",
    "            ideal_size = list(test_data['image_meta_dict']['spatial_shape'][0].cpu().detach().numpy())\n",
    "            print(ideal_size)\n",
    "            # Process the data\n",
    "            segmented = curr_data.cpu()\n",
    "            print(segmented.shape)\n",
    "            upsampled = F.interpolate(segmented, size=[ideal_size[0], segmented.shape[3], segmented.shape[4]], mode='trilinear', align_corners=True)\n",
    "\n",
    "            merged = torch.argmax(upsampled, dim=1) + 1  # add 1 to move the range from 0-5 to 1-6\n",
    "\n",
    "            print(merged.shape)\n",
    "\n",
    "            bounding_boxes_result_v3 = process_and_get_bounding_boxes_v6(merged)\n",
    "\n",
    "\n",
    "            extracted_vol = extract_encompassing_subregion(vol, bounding_boxes_result_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd3e71a-0b24-4666-8da5-885ada442bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|████████████████████████████████████████████████| 44.7M/44.7M [00:00<00:00, 114MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CombinedFeatureExtractor(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (decoder_block1): Sequential(\n",
       "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_block2): Sequential(\n",
       "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (decoder_block3): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc_unet): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (primary_capsules): PrimaryCapsule(\n",
       "    (conv2d): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (digit_capsules): DenseCapsule()\n",
       "  (fc_caps): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the CapsuleFeatureExtractor\n",
    "feature_extractor = CombinedFeatureExtractor(input_channels = 3, pretrained=True, unet_feature_dim=1024, capsnet_feature_dim=1024)\n",
    "feature_extractor.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
